{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs an wasserstein-advarsarial autencoder on our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "> <ipython-input-1-f321063a9bd3>(45)<module>()->None\n",
      "-> pdb.set_trace()\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "import DataProvider as DP\n",
    "import SimpleLogger as SimpleLogger\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "import models.waaegan as waaegan\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "def opt(): 1\n",
    "opt.Diters=5\n",
    "opt.DitersAlt=5\n",
    "opt.gpu_id=0\n",
    "opt.myseed = 0\n",
    "opt.niter=1000\n",
    "opt.noBN=False\n",
    "opt.nlatentdim = 32\n",
    "opt.lrEnc = 0.005\n",
    "opt.lrDec = 0.005\n",
    "opt.lrEncD = 0.00001\n",
    "opt.EncDRatio = 1\n",
    "opt.batch_size = 64\n",
    "opt.nepochs = 250\n",
    "opt.clamp_lower=-0.01\n",
    "opt.clamp_upper=0.01\n",
    "opt.save_dir='./waae/'\n",
    "opt.saveProgressIter = 1\n",
    "opt.saveStateIter = 5\n",
    "\n",
    "pdb.set_trace()\n",
    "\n",
    "if not os.path.exists(opt.save_dir):\n",
    "    os.makedirs(opt.save_dir)\n",
    "\n",
    "DP = importlib.reload(DP)\n",
    "\n",
    "np.random.seed(opt.myseed)\n",
    "\n",
    "image_dir = '/root/data/'\n",
    "\n",
    "opts = {}\n",
    "opts['verbose'] = True\n",
    "opts['pattern'] = '*.tif_flat.png'\n",
    "opts['out_size'] = [64, 64]\n",
    "\n",
    "data_path = './data_' + str(opts['out_size'][0]) + 'x' + str(opts['out_size'][1]) + '.pyt'\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    dp = torch.load(data_path)\n",
    "else:\n",
    "    dp = DP.DataProvider(image_dir, opts)\n",
    "    torch.save(dp, data_path)\n",
    "\n",
    "%matplotlib inline\n",
    "def tensor2img(img):\n",
    "#     img = img / 2 + 0.5 # unnormalize\n",
    "\n",
    "    img = img.numpy()\n",
    "    if img.ndim == 3:\n",
    "        img = np.expand_dims(img, 0)\n",
    "    img = np.transpose(img, [0,2,3,1])\n",
    "    img = np.concatenate(img[:], 1)\n",
    "    return img\n",
    "\n",
    "#     print(img.shape[0]*10)\n",
    "#     print(img.shape[1]*10)    \n",
    "#     fig = plt.figure(figsize = [img.shape[0]/2, img.shape[1]/2])\n",
    "    \n",
    "\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     ax.get_yaxis().set_visible(False)\n",
    "#     ax.imshow(img)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)    \n",
    "    \n",
    "    \n",
    "enc = waaegan.Enc(opt.nlatentdim)\n",
    "dec = waaegan.Dec(opt.nlatentdim)\n",
    "encD = waaegan.EncD(opt.nlatentdim)\n",
    "\n",
    "enc.apply(weights_init)\n",
    "dec.apply(weights_init)\n",
    "encD.apply(weights_init)\n",
    "\n",
    "gpu_id = opt.gpu_id\n",
    "nlatentdim = opt.nlatentdim\n",
    "\n",
    "enc.cuda(gpu_id)\n",
    "dec.cuda(gpu_id)\n",
    "encD.cuda(gpu_id)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optEnc = optim.RMSprop(enc.parameters(), lr=opt.lrEnc)\n",
    "optDec = optim.RMSprop(dec.parameters(), lr=opt.lrDec)\n",
    "optEncD = optim.RMSprop(encD.parameters(), lr=opt.lrEncD)\n",
    "\n",
    "ndat = dp.get_n_train()\n",
    "ndat = 1000\n",
    "\n",
    "logger = SimpleLogger.SimpleLogger(('epoch', 'iter', 'reconLoss', 'minimaxLoss', 'advLoss', 'time'), '[%d][%d] loss: %.6f minimaxLoss: %.6f advLoss: %.6f time: %.2f')\n",
    "\n",
    "one = torch.FloatTensor([1]).cuda(gpu_id)\n",
    "mone = one * -1\n",
    "\n",
    "gen_iterations = 0\n",
    "iteration = 0\n",
    "for epoch in range(1, opt.nepochs+1): # loop over the dataset multiple times\n",
    "\n",
    "    \n",
    "    rand_inds = np.random.permutation(ndat)\n",
    "    inds = (rand_inds[i:i+opt.batch_size] for i in range(0, len(rand_inds), opt.batch_size))\n",
    "    \n",
    "    zAll = list()\n",
    "    \n",
    "    c = 0\n",
    "    for i in inds:\n",
    "        start = time.time()\n",
    "        \n",
    "        c += 1\n",
    "        iteration += 1\n",
    "        \n",
    "        batsize = len(i)\n",
    "\n",
    "        yReal = Variable(torch.ones(batsize)).cuda(gpu_id)\n",
    "        yFake = Variable(torch.zeros(batsize)).cuda(gpu_id)\n",
    "        \n",
    "        ###update the discriminator\n",
    "        #maximize log(AdvZ(z)) + log(1 - AdvZ(Enc(x)))\n",
    "        for p in encD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "\n",
    "\n",
    "        # train the discriminator Diters times\n",
    "        if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            Diters = opt.DitersAlt\n",
    "        else:\n",
    "            Diters = opt.Diters\n",
    "        j = 0\n",
    "\n",
    "        rand_inds_encD = np.random.permutation(ndat)\n",
    "        niter = len(range(0, len(rand_inds_encD), opt.batch_size))\n",
    "        inds_encD = (rand_inds_encD[i:i+opt.batch_size] for i in range(0, len(rand_inds_encD), opt.batch_size))\n",
    "        \n",
    "        while j < Diters and j < niter:\n",
    "            j += 1\n",
    "\n",
    "            # clamp parameters to a cube\n",
    "            for p in encD.parameters():\n",
    "                p.data.clamp_(opt.clamp_lower, opt.clamp_upper)\n",
    "                \n",
    "            x = Variable(dp.get_images(next(inds_encD),'train')).cuda(gpu_id)\n",
    "            \n",
    "            zFake = enc(x)\n",
    "            #pick a distribution that is obvious when you plot it\n",
    "            zReal = Variable(torch.Tensor(batsize, nlatentdim).uniform_(-2, 2)).cuda(gpu_id)\n",
    "            \n",
    "            optEnc.zero_grad()\n",
    "            optEncD.zero_grad()\n",
    "\n",
    "            # train with real\n",
    "            errD_real = encD(zReal)\n",
    "            errD_real.backward(one)\n",
    "\n",
    "            # train with fake\n",
    "            errD_fake = encD(zFake)\n",
    "            errD_fake.backward(mone)\n",
    "            latentLoss = errD_real - errD_fake\n",
    "            optEncD.step()\n",
    "            \n",
    "        optEnc.zero_grad()\n",
    "        optDec.zero_grad()\n",
    "        optEncD.zero_grad()  \n",
    "        \n",
    "#         x = Variable(dp.get_images(i, 'train')).cuda(gpu_id)\n",
    "        \n",
    "        zFake = enc(x)\n",
    "        xHat = dec(zFake)\n",
    "    \n",
    "        reconLoss = criterion(xHat, x)\n",
    "        reconLoss.backward(retain_variables=True)\n",
    "        \n",
    "        for p in encD.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        minimaxLoss = encD(zFake)\n",
    "        minimaxLoss.backward(one*opt.EncDRatio)\n",
    "\n",
    "        optEnc.step()\n",
    "        optDec.step()\n",
    "\n",
    "        zAll.append(zFake.data)\n",
    "        \n",
    "        stop = time.time()\n",
    "        deltaT = stop-start\n",
    "        \n",
    "        logger.add((epoch, iteration, reconLoss.data[0], minimaxLoss.data[0][0], latentLoss.data[0][0], deltaT))\n",
    "\n",
    "    gen_iterations += 1\n",
    "    \n",
    "    if (epoch % opt.saveProgressIter) == 0:\n",
    "\n",
    "        enc.train(False)\n",
    "        dec.train(False)\n",
    "            \n",
    "        x = Variable(dp.get_images(np.arange(0,10),'train')).cuda(gpu_id)\n",
    "        xHat = dec(enc(x))\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "#         out = torchvision.utils.make_grid(x)\n",
    "        imgX = tensor2img(x.data.cpu())\n",
    "        imgXHat = tensor2img(xHat.data.cpu())\n",
    "        \n",
    "        imgOut = np.concatenate((imgX, imgXHat), 0)\n",
    "        \n",
    "        scipy.misc.imsave('./{0}/progress_{1}.png'.format(opt.save_dir, epoch), imgOut)\n",
    "        \n",
    "        enc.train(True)\n",
    "        dec.train(True)\n",
    "        \n",
    "        zAll = torch.cat(zAll,0).cpu().numpy()\n",
    "\n",
    "        plt.gca().cla() \n",
    "        plt.scatter(zAll[:,0], zAll[:,1])\n",
    "        plt.xlim([-4, 4]) \n",
    "        plt.ylim([-4, 4])     \n",
    "        plt.axis('equal')\n",
    "        plt.xlabel('z1')\n",
    "        plt.ylabel('z2')\n",
    "        plt.title('latent space embedding')\n",
    "        plt.savefig('./{0}/embedding_{1}.png'.format(opt.save_dir, epoch), dpi=75)\n",
    "        \n",
    "        pickle.dump(zAll, open('./{0}/embedding.pkl'.format(opt.save_dir), 'wb'))\n",
    "        pickle.dump(logger, open('./{0}/logger.pkl'.format(opt.save_dir), 'wb'))\n",
    "\n",
    "    if (epoch % opt.saveStateIter) == 0:\n",
    "#         for saving and loading see:\n",
    "#         https://discuss.pytorch.org/t/how-to-save-load-torch-models/718\n",
    "        torch.save(enc.state_dict(), './{0}/enc.pth'.format(opt.save_dir))\n",
    "        torch.save(dec.state_dict(), './{0}/dec.pth'.format(opt.save_dir))\n",
    "        torch.save(encD.state_dict(), './{0}/encD.pth'.format(opt.save_dir))\n",
    "        \n",
    "        torch.save(optEnc.state_dict(), './{0}/optEnc.pth'.format(opt.save_dir))\n",
    "        torch.save(optDec.state_dict(), './{0}/optDec.pth'.format(opt.save_dir))\n",
    "        torch.save(optEncD.state_dict(), './{0}/optEncD.pth'.format(opt.save_dir))\n",
    "        \n",
    "        pickle.dump(opt, open('./{0}/opt.pkl'.format(opt.save_dir), 'wb'))\n",
    "            \n",
    "        \n",
    "#     optEnc.param_groups[0]['lr'] = learningRate*(0.999**epoch)\n",
    "#     optDec.param_groups[0]['lr'] = learningRate*(0.999**epoch)\n",
    "#     optEncD.param_groups[0]['lr'] = learningRate*(0.999**epoch)\n",
    "                  \n",
    "                  \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
