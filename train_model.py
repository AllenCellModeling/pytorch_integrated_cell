import argparse

import importlib
import numpy as np

import os
import pickle

import math

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torchvision.utils

import time
import datetime

import warnings
import json

import integrated_cell as ic
from integrated_cell import model_utils

import shutil


# import torch.backends.cudnn as cudnn
# cudnn.benchmark = True

import pdb

def str2bool(v):
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

parser = argparse.ArgumentParser()
parser.add_argument('--Diters', type=int, default=5, help='niters for the encD')
parser.add_argument('--DitersAlt', type=int, default=100, help='niters for the encD')
parser.add_argument('--gpu_ids', nargs='+', type=int, default=0, help='gpu id')
parser.add_argument('--myseed', type=int, default=0, help='random seed')
parser.add_argument('--nlatentdim', type=int, default=16, help='number of latent dimensions')
parser.add_argument('--lrEnc', type=float, default=0.0005, help='learning rate for encoder')
parser.add_argument('--lrDec', type=float, default=0.0005, help='learning rate for decoder')
parser.add_argument('--lrEncD', type=float, default=0.00005, help='learning rate for encD')
parser.add_argument('--lrDecD', type=float, default=0.00005, help='learning rate for decD')

parser.add_argument('--kwargs_optim', type=json.loads, default={}, help='kwargs for optimizer')
parser.add_argument('--kwargs_model', type=json.loads, default={}, help='kwargs for the model')
parser.add_argument('--kwargs_network', type=json.loads, default={}, help='kwargs for the network')

parser.add_argument('--kwargs_enc', type=json.loads, default={}, help='kwargs for the enc')
parser.add_argument('--kwargs_dec', type=json.loads, default={}, help='kwargs for the dec')
parser.add_argument('--kwargs_encD', type=json.loads, default={}, help='kwargs for the encD')
parser.add_argument('--kwargs_decD', type=json.loads, default={}, help='kwargs for the decD')

parser.add_argument('--critRecon', default='BCELoss', help='Loss function for image reconstruction')

parser.add_argument('--batch_size', type=int, default=64, help='batch size')
parser.add_argument('--nepochs', type=int, default=250, help='total number of epochs')
parser.add_argument('--nepochs_pt2', type=int, default=-1, help='total number of epochs')

parser.add_argument('--model_name', default='waaegan', help='name of the model module')
parser.add_argument('--save_dir', type=str, default=None, help='save dir')
parser.add_argument('--save_parent', type=str, default=None, help='parent save directory to save with autogenerated working directory (mutually exclusive to "--save_dir")')
parser.add_argument('--saveProgressIter', type=int, default=1, help='number of iterations between saving progress')
parser.add_argument('--saveStateIter', type=int, default=10, help='number of iterations between saving progress')
parser.add_argument('--data_save_path', default=None, help='save path of data file')
parser.add_argument('--imdir', default='/root/data/release_4_1_17/results_v2/aligned/2D', help='location of images')

parser.add_argument('--latentDistribution', default='gaussian', help='Distribution of latent space, can be {gaussian, uniform}')

parser.add_argument('--ndat', type=int, default=-1, help='Number of data points to use')
parser.add_argument('--optimizer', default='adam', help='type of optimizer, can be {adam, RMSprop}')

parser.add_argument('--train_module', default=None, help='training module')
parser.add_argument('--train_module_pt1', default=None, help='training module')
parser.add_argument('--train_module_pt2', default=None, help='training module')

parser.add_argument('--dataProvider', default='DataProvider', help='Dataprovider object')

parser.add_argument('--channels_pt1', nargs='+', type=int, default=[0,2], help='channels to use for part 1')
parser.add_argument('--channels_pt2', nargs='+', type=int, default=[0,1,2], help='channels to use for part 2')

parser.add_argument('--dtype', default='float', help='data type that the dataprovider uses. Only \'float\' supported.')

parser.add_argument('--overwrite_opts', default=False, type=str2bool, help='Overwrite options file')
parser.add_argument('--skip_pt1', default=False, type=str2bool, help='Skip pt 1')

parser.add_argument('--ref_dir', default='ref_model', type=str, help='Directory name for reference model')
parser.add_argument('--struct_dir', default='struct_model', type=str, help='Directory name for structure model')

opt = parser.parse_args()

skip1 = opt.skip_pt1

ref_dir = opt.ref_dir
struct_dir = opt.struct_dir

os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(ID) for ID in opt.gpu_ids])


if (opt.save_parent is not None) and (opt.save_dir is not None):
    raise ValueError('--save_dir and --save_parent are both set. Please choose one or the other.')
    
if ((opt.train_module is not None) and (opt.train_module_pt1 is not None)) or ((opt.train_module is not None) and (opt.train_module_pt2 is not None)):
    raise ValueError('--train_module and --train_model_pt1 or --train_model_pt2 are both set. Please choose a global train module or specify partial models.')

the_time = datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S")
if opt.save_parent is not None:
    opt.save_dir = os.path.join(opt.save_parent, the_time)

opt.the_time = the_time
opt.save_parent = opt.save_dir

if opt.data_save_path is None:
    opt.data_save_path = opt.save_dir + os.sep + 'data.pyt'

if not os.path.exists(opt.save_parent):
    os.makedirs(opt.save_parent)

opt_save_path = '{0}/opt.pkl'.format(opt.save_parent)

def load_opts(opt):
    opt.opt_save_path = '{0}/opt.pkl'.format(opt.save_dir)

    if os.path.exists(opt.opt_save_path) and not opt.overwrite_opts:
        warnings.warn('Options file exists and overwrite is not set to True. Using existing options file.')

        #load options file
        opt = pickle.load( open( opt.opt_save_path, 'rb' ) )
    else:

        #make a copy if the opts file exists
        if os.path.exists(opt.opt_save_path):
            shutil.copyfile(opt.opt_save_path, '{0}_{1}'.format(opt.opt_save_path, the_time))

        pickle.dump(opt, open(opt.opt_save_path, 'wb'))
    
    print(opt)
    return opt


if opt.train_module is not None:
    opt.train_module_pt1 = opt.train_module
    opt.train_module_pt2 = opt.train_module
    

opt.gpu_ids = list(range(0, len(opt.gpu_ids)))


torch.manual_seed(opt.myseed)
torch.cuda.manual_seed(opt.myseed)
np.random.seed(opt.myseed)


if opt.nepochs_pt2 == -1:
    opt.nepochs_pt2 = opt.nepochs

dp = model_utils.load_data_provider(opt.data_save_path, opt.imdir, opt.dataProvider)

if opt.ndat == -1:
    opt.ndat = dp.get_n_dat('train')


iters_per_epoch = np.ceil(opt.ndat/opt.batch_size)



#######
### TRAIN REFERENCE MODEL
#######

opt.save_dir = os.path.join(opt.save_parent, ref_dir)
if not os.path.exists(opt.save_dir):
    os.makedirs(opt.save_dir)

if not skip1:
    opt = load_opts(opt)

    opt.channelInds = opt.channels_pt1
    dp.opts['channelInds'] = opt.channelInds
    opt.n_channels = len(opt.channelInds)
    opt.n_classes = 0
    opt.n_ref = 0

    model_module = importlib.import_module("integrated_cell.models." + opt.train_module_pt1)
    model = model_module.Model(data_provider = dp,
                                batch_size = opt.batch_size,
                                n_channels = opt.n_channels,
                                n_latent_dim = opt.nlatentdim,
                                n_classes = opt.n_classes,
                                n_ref = opt.n_ref,
                                gpu_ids = opt.gpu_ids,
                                **opt.kwargs_model)


    pickle.dump(opt, open('{0}/opt.pkl'.format(opt.save_dir), 'wb'))

    models, optimizers, criterions, logger = model.load(opt.model_name, opt)

    start_iter = len(logger.log['iter'])
    zAll = list()



    for this_iter in range(start_iter, math.ceil(iters_per_epoch)*opt.nepochs):
        opt.iter = this_iter

        epoch = np.floor(this_iter/iters_per_epoch)
        epoch_next = np.floor((this_iter+1)/iters_per_epoch)

        start = time.time()

        errors, zfake = model.iteration(**models, **optimizers, **criterions, data_provider=dp, opt=opt)

        zAll.append(zfake)

        stop = time.time()
        deltaT = stop-start

        logger.add([epoch, this_iter] + errors + [deltaT])

        if model_utils.maybe_save(model, epoch, epoch_next, models, optimizers, logger, zAll, dp, opt):
            zAll = list()

#######
### DONE TRAINING REFERENCE MODEL
#######

#######
### TRAIN STRUCTURE MODEL
#######

embeddings_path = opt.save_dir + os.sep + 'embeddings.pkl'


if skip1:
    embeddings = model_utils.load_embeddings(embeddings_path, None, dp, opt)
else:
    embeddings = model_utils.load_embeddings(embeddings_path, models['enc'], dp, opt)

models = None
optimizers = None

def get_ref(self, inds, train_or_test='train'):
    inds = torch.LongTensor(inds)
    return self.embeddings[train_or_test][inds]

dp.embeddings = embeddings

# do this thing to bind the get_ref method to the dataprovider object
import types
dp.get_ref = types.MethodType(get_ref, dp)

opt.save_dir = os.path.join(opt.save_parent, struct_dir)
if not os.path.exists(opt.save_dir):
    os.makedirs(opt.save_dir)

opt = load_opts(opt)    
    
opt.channelInds = opt.channels_pt2
dp.opts['channelInds'] = opt.channelInds

opt.n_channels = len(opt.channelInds)
opt.n_classes = dp.get_n_classes()
opt.n_ref = opt.nlatentdim

model_module = importlib.import_module("integrated_cell.models." + opt.train_module_pt2)
model = model_module.Model(data_provider = dp,
                        batch_size = opt.batch_size,
                        n_channels = opt.n_channels,
                        n_latent_dim = opt.nlatentdim,
                        n_classes = opt.n_classes,
                        n_ref = opt.n_ref,
                        gpu_ids = opt.gpu_ids)


pickle.dump(opt, open('{0}/opt.pkl'.format(opt.save_dir), 'wb'))

models, optimizers, criterions, logger = model.load(opt.model_name, opt)

start_iter = len(logger.log['iter'])

zAll = list()
for this_iter in range(start_iter, math.ceil(iters_per_epoch)*opt.nepochs_pt2):
    opt.iter = this_iter

    epoch = np.floor(this_iter/(iters_per_epoch))
    epoch_next = np.floor((this_iter+1)/(iters_per_epoch))

    start = time.time()

    errors, zfake = model.iteration(**models, **optimizers, **criterions, data_provider=dp, opt=opt)

    zAll.append(zfake)

    stop = time.time()
    deltaT = stop-start

    logger.add([epoch, this_iter] + errors + [deltaT,])

    if model_utils.maybe_save(model, epoch, epoch_next, models, optimizers, logger, zAll, dp, opt):
        zAll = list()

print('Finished Training')

embeddings_path = opt.save_dir + os.sep + 'embeddings.pkl'
embeddings = model_utils.load_embeddings(embeddings_path, models['enc'], dp, opt)

#######
### DONE TRAINING STRUCTURE MODEL
#######
