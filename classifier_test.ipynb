{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs a quick classifier on our cell images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64])\n",
      "\n",
      " 4\n",
      " 5\n",
      "[torch.LongTensor of size 2]\n",
      "\n",
      "nlabels: 8\n"
     ]
    }
   ],
   "source": [
    "import DataProvider as DP\n",
    "import importlib\n",
    "# import  random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "myseed = 0\n",
    "DP = importlib.reload(DP)\n",
    "\n",
    "\n",
    "np.random.seed(myseed)\n",
    "\n",
    "\n",
    "\n",
    "image_dir = '/root/images/2016_11_08_Nuc_Cell_Seg_8_cell_lines_V22/processed_aligned/2D/'\n",
    "\n",
    "opts = {}\n",
    "opts['out_size'] = [32, 32]\n",
    "opts['verbose'] = True\n",
    "\n",
    "data_path = './data_64x64.pyt'\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    dp = torch.load(data_path)\n",
    "else:\n",
    "    dp = DP.DataProvider(image_dir, opts)\n",
    "    torch.save(dp, data_path)\n",
    "\n",
    "inds = [1,2]\n",
    "images = dp.get_images(inds, 'train')\n",
    "labels = dp.get_labels(inds, 'train')\n",
    "\n",
    "print(images.size())\n",
    "print(labels)\n",
    "nlabels = dp.get_n_classes()\n",
    "\n",
    "print('nlabels: ' + str(nlabels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dims = list(dp.images[0].size())\n",
    "dims.insert(0, len(inds))\n",
    "\n",
    "images = torch.zeros(tuple(dims))\n",
    "\n",
    "c = 0\n",
    "for i in inds:\n",
    "    images[c] = dp.images[dp.data['train']['inds'][i]].clone()\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64])\n",
      "[2, 3, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "# dp.images[dp.data['train']['inds'][i]].size()\n",
    "# images = torch.FloatTensor(dims[:])\n",
    "print(images.size())\n",
    "\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5 # unnormalize\n",
    "\n",
    "    img = img.numpy()\n",
    "    img = np.transpose(img, [1,2,0])\n",
    "    print(np.max(img))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n",
      "0.509804\n",
      "0.509804\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncvDNMuTgm2XIwTfL0P8DE+MR7ACDEdYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f866072d198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = dp.get_images([100], 'train')\n",
    "print(img.size())\n",
    "print(np.max(img[0].numpy()))\n",
    "imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 16])\n",
      "torch.Size([1, 6, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dat = Variable(torch.rand(1, 1, 16, 16))\n",
    "print(dat.data.size())\n",
    "\n",
    "layer = nn.Conv2d(1, 6, 5, 1, 2)\n",
    "dat_out = layer(dat)\n",
    "\n",
    "print(dat_out.data.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, 1, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, 1, 2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        \n",
    "        self.pool  = nn.MaxPool2d(2,2)     \n",
    "        \n",
    "        self.fc1   = nn.Linear(32*8*8, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.bn3 = nn.BatchNorm2d(32)        \n",
    "        self.bn4 = nn.BatchNorm1d(120)\n",
    "        self.bn5 = nn.BatchNorm1d(84)        \n",
    "        self.bn6 = nn.BatchNorm1d(10)        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))        \n",
    "        x = x.view(-1, 32*8*8)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.bn6(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.118\n",
      "[2] loss: 0.251\n",
      "[3] loss: 0.222\n",
      "[4] loss: 0.166\n",
      "[5] loss: 0.231\n",
      "[6] loss: 0.228\n",
      "[7] loss: 0.213\n",
      "[8] loss: 0.074\n",
      "[9] loss: 0.105\n",
      "[10] loss: 0.076\n",
      "[11] loss: 0.117\n",
      "[12] loss: 0.081\n",
      "[13] loss: 0.068\n",
      "[14] loss: 0.071\n",
      "[15] loss: 0.195\n",
      "[16] loss: 0.068\n",
      "[17] loss: 0.053\n",
      "[18] loss: 0.056\n",
      "[19] loss: 0.067\n",
      "[20] loss: 0.186\n",
      "[21] loss: 0.091\n",
      "[22] loss: 0.099\n",
      "[23] loss: 0.186\n",
      "[24] loss: 0.058\n",
      "[25] loss: 0.184\n",
      "[26] loss: 0.099\n",
      "[27] loss: 0.079\n",
      "[28] loss: 0.187\n",
      "[29] loss: 0.197\n",
      "[30] loss: 0.186\n",
      "[31] loss: 0.097\n",
      "[32] loss: 0.169\n",
      "[33] loss: 0.059\n",
      "[34] loss: 0.070\n",
      "[35] loss: 0.086\n",
      "[36] loss: 0.195\n",
      "[37] loss: 0.077\n",
      "[38] loss: 0.084\n",
      "[39] loss: 0.063\n",
      "[40] loss: 0.083\n",
      "[41] loss: 0.056\n",
      "[42] loss: 0.059\n",
      "[43] loss: 0.048\n",
      "[44] loss: 0.080\n",
      "[45] loss: 0.044\n",
      "[46] loss: 0.072\n",
      "[47] loss: 0.165\n",
      "[48] loss: 0.162\n",
      "[49] loss: 0.039\n",
      "[50] loss: 0.078\n",
      "[51] loss: 0.072\n",
      "[52] loss: 0.093\n",
      "[53] loss: 0.167\n",
      "[54] loss: 0.173\n",
      "[55] loss: 0.033\n",
      "[56] loss: 0.136\n",
      "[57] loss: 0.190\n",
      "[58] loss: 0.167\n",
      "[59] loss: 0.128\n",
      "[60] loss: 0.077\n",
      "[61] loss: 0.030\n",
      "[62] loss: 0.177\n",
      "[63] loss: 0.097\n",
      "[64] loss: 0.055\n",
      "[65] loss: 0.034\n",
      "[66] loss: 0.180\n",
      "[67] loss: 0.081\n",
      "[68] loss: 0.170\n",
      "[69] loss: 0.091\n",
      "[70] loss: 0.170\n",
      "[71] loss: 0.030\n",
      "[72] loss: 0.032\n",
      "[73] loss: 0.167\n",
      "[74] loss: 0.059\n",
      "[75] loss: 0.046\n",
      "[76] loss: 0.070\n",
      "[77] loss: 0.063\n",
      "[78] loss: 0.072\n",
      "[79] loss: 0.073\n",
      "[80] loss: 0.068\n",
      "[81] loss: 0.084\n",
      "[82] loss: 0.188\n",
      "[83] loss: 0.082\n",
      "[84] loss: 0.148\n",
      "[85] loss: 0.039\n",
      "[86] loss: 0.068\n",
      "[87] loss: 0.048\n",
      "[88] loss: 0.062\n",
      "[89] loss: 0.026\n",
      "[90] loss: 0.042\n",
      "[91] loss: 0.097\n",
      "[92] loss: 0.078\n",
      "[93] loss: 0.055\n",
      "[94] loss: 0.173\n",
      "[95] loss: 0.086\n",
      "[96] loss: 0.039\n",
      "[97] loss: 0.140\n",
      "[98] loss: 0.072\n",
      "[99] loss: 0.172\n",
      "[100] loss: 0.030\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # use a Classification Cross-Entropy loss\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "batchsize = 64\n",
    "\n",
    "gpu_id = 1\n",
    "net.cuda(gpu_id)\n",
    "\n",
    "\n",
    "for epoch in range(100): # loop over the dataset multiple times\n",
    "\n",
    "    rand_inds = np.random.permutation(dp.get_n_train())\n",
    "    inds = (rand_inds[i:i+batchsize] for i in range(0, len(rand_inds), batchsize))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    c = 0\n",
    "    for i in inds:\n",
    "        c += 1\n",
    "        \n",
    "        inputs = dp.get_images(i, 'train')\n",
    "        labels = dp.get_labels(i, 'train', 'index')\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs).cuda(gpu_id), Variable(labels).cuda(gpu_id)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(loss.data[0])\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "    print('[%d] loss: %.3f' % (epoch+1, running_loss / c))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inds = [i for i in range(0,dp.get_n_test())]\n",
    "images = dp.get_images(inds, 'test').cuda(gpu_id)\n",
    "labels = dp.get_labels(inds, 'test')\n",
    "outputs = net(Variable(images))\n",
    "\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  1  0  0  0  0]\n",
      " [ 1 10  1  0  0  0  1  1]\n",
      " [ 0  4  4  0  0  0  0  1]\n",
      " [ 0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0 11  1  0  0]\n",
      " [ 1  1  0  0  0 13  0  0]\n",
      " [ 0  0  1  0  0  0  5  0]\n",
      " [ 3  0  1  0  0  1  0 16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # use a Classification Cross-Entropy loss\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "batchsize = 64\n",
    "\n",
    "gpu_id = 1\n",
    "net.cuda(gpu_id)\n",
    "\n",
    "\n",
    "rand_inds = np.random.permutation(dp.get_n_train())\n",
    "inds = (rand_inds[i:i+batchsize] for i in range(0, len(rand_inds), batchsize))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1109,  342,  381,  511, 1605,  853,  685,  425,  539,  557, 1491,\n",
       "         271,  516, 1103,  520,  522,  229, 1170, 1576, 1399,  899, 1088,\n",
       "         722,  240, 1648,   76, 1226,  987,  792, 1594,  107,  489,  576,\n",
       "        1073, 1015, 1393, 1124,  461,  593,  360,  438,  621,   80, 1129,\n",
       "         175, 1601,  481,  361,  725, 1479, 1178,  546,  883, 1276,   14,\n",
       "          53, 1122, 1149,  740, 1583,  877, 1532,  708,  943]),\n",
       " array([1553, 1238,   18, 1002,  124, 1621,  299,  984,  308,  914, 1183,\n",
       "         513,  820,  310, 1089, 1034, 1317, 1302,  187,  390, 1154,  414,\n",
       "         655, 1321, 1341, 1179,  905, 1595,  789, 1044,  969,  826,  651,\n",
       "        1264, 1481, 1014,  916,  532,  530,  536,  408, 1655,  658, 1074,\n",
       "        1598,  844,  278,  217, 1259, 1116, 1423,    5,   58, 1575, 1072,\n",
       "        1057,  432,  249,   34, 1169, 1619,  191,  688, 1054]),\n",
       " array([ 609,  161, 1125, 1632,  788,  980, 1287,  452,   19, 1246, 1268,\n",
       "          54, 1450,  771, 1490,  427, 1625,    9,  378,  182,  113, 1194,\n",
       "        1097, 1218,   82, 1288, 1081, 1461, 1533,  575,   66,   85,  579,\n",
       "          92,  817,  122,  223,  733, 1454,  436,   17,  303,  446,   52,\n",
       "         473,  353,  483,  506,  723, 1376,  254, 1230,  156, 1388,  159,\n",
       "         578,  831,  456,  259, 1530, 1267, 1256, 1602, 1547]),\n",
       " array([1445,  268, 1137, 1285,  619,  746,  412, 1578,  326,  895,  745,\n",
       "        1460, 1018,  838,  141, 1148,  253,  737, 1032,  892,  443, 1592,\n",
       "        1063,    4,  204,  878,  152,  517,  867,  233, 1023, 1636, 1140,\n",
       "        1273,  759, 1026,  666,  983,  379, 1542,  728,  219, 1188, 1228,\n",
       "        1580, 1184,   31,  620,  572,  479,  762, 1628, 1586,  937,  148,\n",
       "         712,  847,  776, 1279, 1470, 1069,  923,  785,  347]),\n",
       " array([ 953, 1224,  434,   89, 1442,  440, 1348, 1217,  177, 1534,  704,\n",
       "         942, 1554, 1177, 1027,  582,  298, 1362,  581, 1236,  898, 1095,\n",
       "        1662,  171, 1133,  270,  227,  538,  519, 1425, 1209,  491, 1310,\n",
       "         317, 1604,  192,  260,  570,  713, 1161,  958, 1175, 1262, 1368,\n",
       "        1508, 1651, 1114,  487, 1211, 1526, 1564,  133,  202, 1058,  896,\n",
       "         527, 1055,  467,  142, 1248,  744, 1356,  315,  563]),\n",
       " array([ 170, 1001,  935,  881, 1573,  458,   22, 1429,   45, 1239, 1353,\n",
       "          61,  501, 1432,  319, 1326,  602,  393,  814, 1518, 1335, 1408,\n",
       "          27,  580, 1087,  289,  861,  564,  649,  210,  654, 1227,  587,\n",
       "        1654,  475,    2, 1271,  999,  858, 1629,  211,  618, 1447, 1242,\n",
       "        1428, 1292, 1416,  840,  459, 1412,   96,  178,  108, 1587, 1245,\n",
       "        1265,  616,  646,    1, 1524,  567, 1378,  535,   48]),\n",
       " array([ 918, 1627, 1369, 1220, 1606, 1357,  982, 1503, 1314,  829,  231,\n",
       "         724,  940, 1092, 1344,  215, 1555, 1039,  354,   55,  794, 1086,\n",
       "         668,  678,  279,  768, 1505,  224,  302, 1332,  218,  810,  912,\n",
       "         442,  825, 1315,  372,  295,  364, 1471, 1038,  526,  402,  568,\n",
       "         333, 1516,  948,  399, 1630, 1436,  720, 1612,  542,  670,  642,\n",
       "         904,  549,  775,  813,  558,  667,  957,  465,  874]),\n",
       " array([ 529,  565, 1480,  276,  716,  184,  796, 1225, 1281, 1545, 1110,\n",
       "        1515,  376, 1291, 1449,  597, 1544,   42, 1347,  986, 1128, 1367,\n",
       "        1665, 1562,  198,  309,  818, 1649,  574,   47, 1234,  554,  995,\n",
       "         220,  615,  875,  679, 1062,  362, 1337,  420,  711,  512, 1061,\n",
       "         140, 1031,  793, 1363,   77, 1083,   39,  913,  933,  841, 1280,\n",
       "        1634,  322,  994,  251,  672, 1597,  879,  482,  864]),\n",
       " array([ 996,  823,  114,  241,  330, 1657,   56,  897,  411,  435, 1433,\n",
       "        1584,  884, 1078,  717,   29,   35, 1182,  422, 1338, 1509, 1406,\n",
       "        1096, 1042, 1193,  524, 1391, 1270, 1195, 1529, 1513,  731, 1235,\n",
       "         553,  644,  648,  548, 1261,  494,  693,  431, 1101,  665, 1064,\n",
       "        1439,  924,   40, 1333,   75, 1105,  891,  247,  743, 1631,  186,\n",
       "         674,  312,    8, 1135, 1274, 1600,  799,  757, 1185]),\n",
       " array([ 808, 1005,  445,  900,  464, 1336,  195, 1024,  781,  960, 1541,\n",
       "         243,  401, 1488,  689,  748, 1487,  477, 1497, 1582,  887,  285,\n",
       "         471,  703,  977,  971, 1056,  632, 1551,  426, 1339,  686,  162,\n",
       "        1528,  657, 1444, 1319, 1025, 1603,  194,  474, 1463,  569, 1190,\n",
       "          49,  641,  801, 1499,  726,  784,  761, 1360,   44, 1372,  245,\n",
       "         985,  300,  583, 1263,  269,  638,  313, 1229,  283]),\n",
       " array([1158,  540,  406,  338,  466,  363,  692,  320, 1645, 1411,   70,\n",
       "        1049,  500,   10, 1258,  505,  930, 1010, 1467,   64, 1145, 1581,\n",
       "         416,  938,  752,  205,  328,  486, 1036, 1410, 1013,  528, 1596,\n",
       "         656,  418,  862, 1327,  760,  590, 1247,  989,  472, 1501,  131,\n",
       "          15,  439, 1566,  476, 1330, 1070, 1132, 1282, 1398,  103, 1191,\n",
       "          60,  795,   78,  391,  165, 1286,  970, 1456, 1003]),\n",
       " array([ 349,  185, 1325,  371,  117, 1257,  252,  507, 1663,  311, 1561,\n",
       "         395,  955,  695,  358, 1661,  263,  359,  927, 1168, 1543, 1409,\n",
       "        1482,  634, 1590,   16,   51, 1283,  493,  601, 1440,  351,  921,\n",
       "        1643, 1523, 1174,  871,   97, 1277, 1458,  196, 1527,  183, 1468,\n",
       "        1618,  613,  906, 1030,  682,  533,  610,  687, 1249,   81,  936,\n",
       "         397,  267,  589,  386,  828, 1451,  132,  265,  997]),\n",
       " array([1381,  963,   88, 1301, 1253, 1476,  676, 1478,  629, 1294,  966,\n",
       "         294,  261, 1448,  863, 1379, 1615,  405, 1656,  356,  608, 1189,\n",
       "         239, 1401,  907, 1187,  119, 1029, 1298, 1560,  751,  374,  453,\n",
       "         110, 1394, 1424, 1414, 1385,  503, 1567, 1223,  974,  729,  410,\n",
       "        1093,  367,  805, 1647,  384,  200, 1403,   65, 1389,  992, 1045,\n",
       "         272, 1453, 1438,  677,  238, 1157, 1017,  541,  478]),\n",
       " array([1342, 1290, 1502, 1196, 1355,  188, 1579,  681,  946, 1115,  911,\n",
       "         316,  214, 1100, 1539, 1364,  660, 1413,  531,  255,   30,  662,\n",
       "        1402,  230,  157, 1374,  385, 1121,  380, 1213,  721,  264,  787,\n",
       "         101,   13, 1299,   68, 1166, 1136, 1181,   71, 1616, 1644,  566,\n",
       "        1452, 1050,  798,  846,  523,  626,  495,  979,  155, 1199,  758,\n",
       "         962,  866,  790, 1380, 1255,  366, 1343, 1569, 1079]),\n",
       " array([ 839,  228,  417,  206,  742,   79,  190,  492,  991, 1052,  145,\n",
       "         965,  318,  824, 1546, 1147, 1531,  596,  457,  880,  144, 1390,\n",
       "        1318, 1525,  764,  842, 1164, 1443, 1658,  944,  592, 1495,  135,\n",
       "        1150,   83, 1084,  154, 1165, 1309,  773, 1021,  127, 1202, 1244,\n",
       "         961,  403,  150,  346,  413, 1457, 1142, 1108,  394,  981, 1076,\n",
       "         208,  496,  352,   95,  625,  571,   37,  485,  959]),\n",
       " array([ 143,  988,  158, 1500,  552, 1565, 1051,  518, 1205, 1589,  236,\n",
       "        1123,  222, 1599,  345, 1659,  120,  661,  588, 1664,  993,   62,\n",
       "        1464,  811,  193,  235,  181, 1349,  389,  968, 1126,  521,  669,\n",
       "         332,  258, 1303, 1180, 1000,  382, 1415, 1549,  331,  934,  614,\n",
       "         262,  893,    6,  266,  769,   38,  901, 1232, 1617,  545,  118,\n",
       "         783,  652, 1047,  706,  105, 1214,  484, 1608,  630]),\n",
       " array([ 683,   20,  870,  242,   50,  915,  441,  707, 1099, 1197,  822,\n",
       "         301,  172,  284,  920, 1540,  344,  306, 1633,  922, 1473,  167,\n",
       "         421,  225,   12, 1254,   90, 1269, 1375,  857, 1421, 1652,  304,\n",
       "        1295, 1611,  490,   74,  515,  561,  179,  109,  116,  293,  643,\n",
       "         772, 1384, 1572,  622, 1358,  409,  636, 1639,  288, 1477,  337,\n",
       "        1212, 1577, 1012,   46,  747,  753,  462, 1474, 1091]),\n",
       " array([1351,  424,  325,  664,  350,  447, 1200, 1041, 1186, 1556,  754,\n",
       "         138,  451,  699, 1426,  830, 1102,  950, 1019, 1307,  396, 1305,\n",
       "        1275, 1419,   21,  766,  392,  250, 1240, 1059,  671, 1642, 1322,\n",
       "        1146,  869,  604, 1151, 1127,  627,  902,  856,  700, 1222,  709,\n",
       "        1635,  321, 1484, 1422,  624,  125, 1011, 1090,  848,  780,  605,\n",
       "        1646, 1382,  415,  100,  741,  334, 1366, 1387,  834]),\n",
       " array([ 833, 1538,  939, 1459, 1563, 1210, 1203,  640,  104,  815,  998,\n",
       "        1075, 1568,  460, 1046,  585, 1465, 1192,  497,  702,  375,  975,\n",
       "        1296,  404,  419, 1361, 1156,  173,  690,  827,  428, 1237, 1139,\n",
       "        1441,  718,  282, 1504, 1120, 1266,  635,  738,  499, 1098, 1113,\n",
       "         691,    0, 1173,  926, 1131, 1558, 1320,  327,  102,  213, 1159,\n",
       "         852,  246, 1641, 1475, 1507, 1260, 1404,  365,  248]),\n",
       " array([ 736,   26,  836, 1511,  680, 1492,    3,  134,  837,  355,  292,\n",
       "         698, 1048, 1427, 1517,  369, 1377,  226,  339, 1437,  603,  454,\n",
       "          63,  990,  237,  650,  112, 1638, 1396,  577,  126, 1008, 1430,\n",
       "        1328,  851,  694,  137, 1009, 1354,    7,  502,  854,  947,  631,\n",
       "         929, 1386,  407, 1623,  357,  153,  343,  305, 1493,  612, 1311,\n",
       "         280, 1498, 1613, 1080,  400,  468, 1233,  504,  534]),\n",
       " array([1016,  903, 1334,  550, 1486, 1359, 1130,  945,  163,  336,   59,\n",
       "        1118, 1557, 1607,  449,  855, 1324, 1323, 1489,  437,  786,  873,\n",
       "        1537,  819, 1243, 1006,  586, 1306,  941,  348, 1370, 1028, 1163,\n",
       "         463, 1231, 1160, 1559,  653,  573,  647,  595,  890, 1571, 1300,\n",
       "        1308,  470,  800,  281,  932,  732,  715,  832,  951, 1144, 1037,\n",
       "        1340,  455,  584,  147,  160, 1111,  750, 1117,  859]),\n",
       " array([ 952, 1400, 1331, 1219, 1215,  628,  606,  164,  106, 1040,  617,\n",
       "        1397,  329,  480,  498, 1552, 1431,   93,  809,  433, 1107,   43,\n",
       "         860, 1462,  967,  710, 1626, 1365, 1588, 1346,  765,  885,  556,\n",
       "        1585,   69,  514,  509,  340,  221, 1297,  146, 1313, 1053, 1138,\n",
       "         803,  508, 1494,  176,  804,  168, 1371, 1446, 1392, 1535,  129,\n",
       "         189,  136, 1520, 1278,  894,  197,  290,   36,  767]),\n",
       " array([ 232,  645,   33, 1112, 1610, 1068,  865,  341, 1085,  199,  429,\n",
       "        1418, 1640,  876,  730,  919,  297,   73, 1417,  807,  547,  949,\n",
       "        1550, 1514, 1521,  821,  663, 1241, 1506,  623, 1162,  212, 1035,\n",
       "        1519, 1548,  727,  701, 1022, 1593,  234, 1007,  978,   67,   24,\n",
       "        1405,  216,  812,  735, 1153, 1373,  111,  166, 1591,  207,  274,\n",
       "          41,  591, 1614,  525,  287,  469, 1350,  121, 1660]),\n",
       " array([1252,  928,  673, 1469, 1141,  886,  956,   25, 1134,  149,  925,\n",
       "        1176,  696,  843,  889, 1485,  598, 1020,  139,  749, 1284, 1004,\n",
       "         323,  180, 1272,  450, 1434,  607, 1043, 1352,  296, 1293, 1250,\n",
       "          94, 1522,  637, 1304,  286,  488,  444, 1208, 1395, 1198,  275,\n",
       "         806, 1152, 1206,   32, 1104,  307,  770,   11,  872, 1067,   86,\n",
       "         778, 1060, 1650, 1082, 1065,  782, 1435,  562,  209]),\n",
       " array([1172,  739,  594,  123,  734,  931,   98,  377,  130,   23, 1407,\n",
       "        1570,  555,  370, 1536,  383,  201,  917,  368,  973,  387,  816,\n",
       "        1316,  256, 1221, 1119, 1472,  169, 1329,  560,  373,  909,  719,\n",
       "         791,  954, 1251, 1167, 1204,  868,  972, 1155, 1622, 1071,  964,\n",
       "         774,  324,  203,   84,  633,  908,  611,  398,   91, 1106,  430,\n",
       "         779, 1143,  291,   57, 1345,  257,  888, 1620, 1466]),\n",
       " array([1066,  388,  335,  273,  756, 1512, 1574, 1077, 1609,  128,  802,\n",
       "          28, 1207,  882, 1483,  510,  675,  151,  244,  714,  543,  544,\n",
       "         639,  697, 1289, 1312,  423,  910, 1171,  659,  797, 1637, 1201,\n",
       "          99,  850,  448, 1455,  755,  976,  115,  777,   72,  845,  537,\n",
       "         849, 1624,  174,   87,  551, 1510,  705,  314, 1420,  600, 1496,\n",
       "        1094,  599,  277, 1033, 1383,  763,  835, 1216, 1653]),\n",
       " array([559, 684])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
