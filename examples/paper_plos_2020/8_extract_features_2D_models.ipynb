{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D model feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect whether the script is running in Jupyter or in\n",
    "# batch mode\n",
    "def fnIsBatchMode(argDebug = False):\n",
    "    try:\n",
    "        get_ipython\n",
    "        \n",
    "    except:\n",
    "        blnBatchMode = True\n",
    "        if (argDebug): print('Batch mode detected')\n",
    "        \n",
    "    else:\n",
    "        blnBatchMode = False\n",
    "        print('Interactive mode detected')\n",
    "        \n",
    "    return blnBatchMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (not fnIsBatchMode()):\n",
    "    # Automatically reload modules before code execution\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "strTimeZone = 'America/Los_Angeles'\n",
    "\n",
    "# Returns the current datetime\n",
    "def fnNow():\n",
    "    return datetime.datetime.now(timezone(strTimeZone))\n",
    "\n",
    "# Returns a timestamp (for use in filenames) in the format:\n",
    "# YYYY/MM/DD-HH:MM:SS\n",
    "#def fnGenTimestamp(argDatetime = fnNow()):  # TODO: For some reason this doesn't work\n",
    "def fnGenTimestamp(argDatetime = None):\n",
    "    if (argDatetime == None):\n",
    "        argDatetime = fnNow()\n",
    "    return argDatetime.strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Returns a date and time in human-readable format\n",
    "#def fnGetDatetime(argDatetime = fnNow()):  # TODO: For some reason this doesn't work\n",
    "def fnGetDatetime(argDatetime = None):\n",
    "    if (argDatetime == None):\n",
    "        argDatetime = fnNow()\n",
    "    return argDatetime.strftime('%m/%d/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Checks to see if a specified directory exists. If not, create a new one\n",
    "def fnOSMakeDir(argPath):\n",
    "    if (not os.path.exists(argPath)):\n",
    "        print('The specified directory does not exist. Creating a new one: {}'.format(argPath))\n",
    "        os.mkdir(argPath)\n",
    "        \n",
    "def fnSplitFullFilename(argFullFilename, argDebug = False):\n",
    "    strPath, strFilename = os.path.split(argFullFilename)\n",
    "    strBasename, strExt = os.path.splitext(strFilename)\n",
    "    \n",
    "    return strPath, strBasename, strExt\n",
    "\n",
    "# Returns a list of full filenames recursively from a parent directory,\n",
    "# sorted and filtered based on argFileExt\n",
    "def fnGetFullFilenames(argParentPath, argFileExt, argDebug = False):\n",
    "    lstFullFilenames = []\n",
    "    \n",
    "    # Loop recursively using argParentPath as the starting point\n",
    "    for strRoot, lstDirs, lstFilenames in os.walk(argParentPath):\n",
    "        if (argDebug):\n",
    "            print('strRoot = {}'.format(strRoot))\n",
    "            print('lstDirs = {}'.format(lstDirs))\n",
    "            print('lstFilenames = {}'.format(lstFilenames))\n",
    "            print()\n",
    "            \n",
    "        if (len(lstFilenames) > 0):\n",
    "            lstFilenames.sort()\n",
    "            \n",
    "            # Loop through the list of filenames and reconstruct\n",
    "            # with their full paths\n",
    "            for strFilename in lstFilenames:\n",
    "                # Process only files with extension argFileExt\n",
    "                if strFilename.endswith(argFileExt):\n",
    "                    # Append full path to each filename\n",
    "                    strFullFilename = os.path.join(strRoot, strFilename)\n",
    "                    lstFullFilenames.append(strFullFilename)\n",
    "                    if (argDebug): print('  {}'.format(strFullFilename))\n",
    "                    \n",
    "    lstFullFilenames.sort()\n",
    "    \n",
    "    return lstFullFilenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "\n",
    "from integrated_cell import model_utils, utils\n",
    "from integrated_cell.metrics.embeddings_reference import get_latent_embeddings\n",
    "from integrated_cell.models.bvae import kl_divergence\n",
    "\n",
    "\n",
    "def dim_klds(mus, sigmas):\n",
    "    kl_dims = list()\n",
    "    for mu, sigma in zip(mus, sigmas):\n",
    "        _, kl_dim, _ = kl_divergence(mu.unsqueeze(0), sigma.unsqueeze(0))\n",
    "        \n",
    "        kl_dims.append(kl_dim)\n",
    "    \n",
    "    return np.vstack(np.vstack(kl_dims))    \n",
    "    \n",
    "    \n",
    "def get_embeddings_for_model(suffix, model_dir, parent_dir, save_path, use_current_results, mode = \"validate\"):\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        if use_current_results:\n",
    "            return None\n",
    "            \n",
    "        networks, dp, args = utils.load_network_from_dir(model_dir, parent_dir, suffix = suffix)\n",
    "\n",
    "        recon_loss = utils.load_losses(args)['crit_recon']\n",
    "\n",
    "        enc = networks['enc']\n",
    "        dec = networks['dec']\n",
    "\n",
    "        enc.train(False)\n",
    "        dec.train(False)\n",
    "\n",
    "        embeddings = get_latent_embeddings(enc, dec, dp, modes=[mode], recon_loss = recon_loss, batch_size = 32)\n",
    "\n",
    "        torch.save(embeddings, save_path)\n",
    "    else:\n",
    "        embeddings = torch.load(save_path)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def embeddings2elbo(embeddings, alpha=0.5, mode = \"validate\"):\n",
    "\n",
    "    recon_per_point = torch.mean(embeddings[mode]['ref']['recon'], 1)\n",
    "    kld_per_point =  embeddings[mode]['ref']['kld']\n",
    "    \n",
    "    elbo_per_point = -2*((1-alpha)*recon_per_point + alpha*kld_per_point)\n",
    "    \n",
    "    return elbo_per_point, recon_per_point, kld_per_point\n",
    "\n",
    "\n",
    "def get_embeddings_for_dir(model_dir, parent_dir, use_current_results=False, suffixes = None, mode = 'validate'):\n",
    "    model_paths = np.array(natsorted(glob.glob('{}/ref_model/enc_*'.format(model_dir))))\n",
    "    \n",
    "    inds = np.linspace(0, len(model_paths)-1).astype('int')\n",
    "    \n",
    "    model_paths = model_paths[inds]\n",
    "    \n",
    "    if suffixes is None:\n",
    "        suffixes = [model_path.split('/enc')[1].split('.pth')[0] for model_path in model_paths]\n",
    "    \n",
    "    results_dir = '{}/results'.format(model_dir)\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    \n",
    "    embeddings_list = list()\n",
    "    \n",
    "    logger_file = '{0}/ref_model/logger_tmp.pkl'.format(model_dir)\n",
    "    \n",
    "    if not os.path.exists(logger_file):\n",
    "        return\n",
    "    \n",
    "    with open( logger_file, \"rb\" ) as fp:\n",
    "        logger = pickle.load(fp)\n",
    "\n",
    "    args_file = \"{}/args.json\".format(model_dir)\n",
    "    with open(args_file, \"r\") as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    model_summaries = list()\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        \n",
    "        model_summary_path = \"{}/ref_model/embeddings_{}{}_summary.pth\".format(model_dir, mode, suffix)\n",
    "        \n",
    "#         if os.path.exists(model_summary_path):\n",
    "#             with open(model_summary_path, \"rb\") as f:\n",
    "#                 model_summary = pickle.load(f)\n",
    "#         else:\n",
    "        embeddings_path = \"{}/ref_model/embeddings_{}{}.pth\".format(model_dir, mode, suffix)\n",
    "    \n",
    "        embeddings = get_embeddings_for_model(suffix, model_dir, parent_dir, embeddings_path, use_current_results, mode = mode)\n",
    "\n",
    "        if embeddings is None: continue\n",
    "\n",
    "        opt = json.load(open( '{0}/args.json'.format(model_dir), \"rb\" ))\n",
    "\n",
    "        iteration = int(suffix[1:])-1\n",
    "        iteration_index = np.where(np.array(logger.log['iter']) == iteration)[0]\n",
    "\n",
    "        if len(iteration_index) == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        embeddings['beta'] = opt['kwargs_model']['alpha']\n",
    "        embeddings['elbo'], embeddings['recon'], embeddings['kld'] = embeddings2elbo(embeddings, embeddings['beta'], mode = mode)\n",
    "\n",
    "        klds_per_dim = dim_klds(embeddings[mode]['ref']['mu'], embeddings[mode]['ref']['sigma'])\n",
    "\n",
    "        model_summary = {\"iteration\": iteration,\n",
    "                \"epoch\": np.array(logger.log['epoch'])[iteration_index],\n",
    "                \"elbo\": np.mean(embeddings['elbo'].numpy()),\n",
    "                \"recons\": np.mean(embeddings['recon'].numpy()),\n",
    "                \"klds\": np.mean(embeddings['kld'].numpy()),\n",
    "                \"klds_per_dim\": np.mean(klds_per_dim, 0),\n",
    "                \"model_dir\": model_dir,\n",
    "                \"label\": model_dir.split('/')[-2],\n",
    "                \"suffix\": suffix,\n",
    "                \"args\": args}\n",
    "\n",
    "        with open(model_summary_path, \"wb\") as f:\n",
    "            pickle.dump(model_summary, f)\n",
    "\n",
    "        model_summaries.append(model_summary)\n",
    "            \n",
    "    return model_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not fnIsBatchMode()):\n",
    "    # Set plotting style\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_ids = [5]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(ID) for ID in gpu_ids])\n",
    "if len(gpu_ids) == 1:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "parent_dir = \"/allen/aics/modeling/gregj/results/integrated_cell/\"\n",
    "\n",
    "model_parent = '{}/test_cbvae_beta_ref'.format(parent_dir)\n",
    "\n",
    "#model_dirs = glob.glob('/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_*/')\n",
    "\n",
    "model_dirs = [\n",
    "    '/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_298/', \n",
    "    '/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_299/', \n",
    "    \n",
    "    '/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_312/', \n",
    "    '/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_313/', \n",
    "    \n",
    "    #'/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_329/', \n",
    "    \n",
    "    '/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_330/', \n",
    "    '/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_331/', \n",
    "    \n",
    "    # All generated cells look the same since beta is too high, not a good model to use\n",
    "    #'/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_378/', \n",
    "    #'/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae_beta_ref/job_379/', \n",
    "]\n",
    "\n",
    "        \n",
    "save_dir = '{}/results'.format(model_parent)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "results_dir = save_dir\n",
    "    \n",
    "    \n",
    "\n",
    "datStart = fnNow()\n",
    "print(f'Started on {fnGetDatetime(datStart)}')\n",
    "print()\n",
    "\n",
    "data_list = list()\n",
    "for i, model_dir in enumerate(model_dirs):\n",
    "    print(model_dir)\n",
    "    \n",
    "    # do model selection based on validation data\n",
    "    model_summaries = get_embeddings_for_dir(model_dir, parent_dir, use_current_results = False, mode='validate')\n",
    "\n",
    "    if model_summaries is None:\n",
    "        continue\n",
    "        \n",
    "    # find the best model    \n",
    "    elbo = np.array([model_summary['elbo'] for model_summary in model_summaries])\n",
    "    suffix = [model_summary['suffix'] for model_summary in model_summaries] \n",
    "    \n",
    "    if len(elbo) == 0:\n",
    "        continue\n",
    "    \n",
    "    max_ind = np.argmax(elbo)\n",
    "    best_elbo = elbo[max_ind]\n",
    "    best_suffix = suffix[max_ind]\n",
    "    \n",
    "    best_ind = int(max_ind)\n",
    "    \n",
    "    # get results for test data\n",
    "    model_summaries = get_embeddings_for_dir(model_dir, parent_dir, use_current_results = False, mode = \"test\", suffixes=[best_suffix])\n",
    "    \n",
    "    iteration = np.array([model_summary['iteration'] for model_summary in model_summaries])\n",
    "    epoch = np.array([model_summary['epoch'] for model_summary in model_summaries])\n",
    "    elbo = np.array([model_summary['elbo'] for model_summary in model_summaries])\n",
    "    recons = np.array([model_summary['recons'] for model_summary in model_summaries])\n",
    "    klds = np.array([model_summary['klds'] for model_summary in model_summaries])\n",
    "    args = [model_summary['args'] for model_summary in model_summaries]\n",
    "    suffix = [model_summary['suffix'] for model_summary in model_summaries]    \n",
    "    klds_per_dim = np.hstack([model_summary['klds_per_dim'] for model_summary in model_summaries])\n",
    "    \n",
    "    beta = args[0]['kwargs_model']['alpha']\n",
    "    \n",
    "    label = model_dir.split('/')[-2]\n",
    "    \n",
    "    model_summary = {\"iteration\": iteration,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"elbo\": elbo,\n",
    "                    \"recons\": recons,\n",
    "                    \"klds\": klds,\n",
    "                    \"klds_per_dim\": klds_per_dim,\n",
    "                    \"model_dir\": model_dir,\n",
    "                    \"label\": label,\n",
    "                    \"suffix\": suffix,\n",
    "                    \"args\": args,\n",
    "                    \"best_elbo\": best_elbo,\n",
    "                    \"beta\": beta}\n",
    "    \n",
    "\n",
    "    data_list.append(model_summary)\n",
    "\n",
    "datEnd = fnNow()\n",
    "print()\n",
    "print(f'Ended on {fnGetDatetime(datEnd)}')\n",
    "\n",
    "datDuration = datEnd - datStart\n",
    "print(f'datDuration = {datDuration}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Num. models = {len(data_list)}, {type(data_list[0])}')\n",
    "\n",
    "lstBeta = []\n",
    "\n",
    "for dctModel in data_list:\n",
    "    print(f\"suffix = {dctModel['suffix']}: beta = {dctModel['beta']}\")\n",
    "    lstBeta.append(dctModel['beta'])\n",
    "    \n",
    "print(f'Sorted betas = {np.sort(lstBeta)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize the data structure into something slightly more manageable \n",
    "\n",
    "import tqdm \n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "\n",
    "from skimage.external.tifffile import imsave\n",
    "\n",
    "ks = list(range(1,11))\n",
    "\n",
    "cuda = True\n",
    "\n",
    "# dims = 2048\n",
    "# block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "# model = InceptionV3([block_idx])\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "\n",
    "#inception score stuff\n",
    "# inception_dir = '{}/results/inception/'.format(model_parent)\n",
    "\n",
    "#Sample a generated and real images into their own class folders\n",
    "modes = ['train','test','validate']\n",
    "\n",
    "im_paths_real = {}\n",
    "im_scores_real = {}\n",
    "im_paths_gen = {}\n",
    "\n",
    "class_list = list()\n",
    "path_list = list()\n",
    "mode_list = list()\n",
    "\n",
    "_, dp, _ = utils.load_network_from_dir(data_list[0]['model_dir'], parent_dir)\n",
    "dp.image_parent = '/allen/aics/modeling/gregj/results/ipp/scp_19_04_10/'\n",
    "\n",
    "class_list = np.array(class_list)\n",
    "path_list = np.array(path_list)\n",
    "mode_list = np.array(mode_list)\n",
    "\n",
    "class_list_gen = class_list[mode_list == 'validate']\n",
    "\n",
    "im_paths_gen = {}\n",
    "im_scores_gen = {}\n",
    "\n",
    "#sample n_train images and stick them into directories\n",
    "for i, data in enumerate(data_list):    \n",
    "\n",
    "    model_ind = 0\n",
    "    \n",
    "    if len(data['suffix']) == 0:\n",
    "        continue\n",
    "        \n",
    "    #Make sure we get the hightest-ELBO model\n",
    "        \n",
    "    suffix = data['suffix'][model_ind]\n",
    "    model_dir = data['model_dir']\n",
    "    model_short = data['model_dir'].split('/')[-2]\n",
    "\n",
    "    im_paths_gen[i] = {}\n",
    "    im_scores_gen[i] = {}\n",
    "    \n",
    "    im_scores_gen[i]['model_dir'] = data['model_dir']\n",
    "    im_scores_gen[i]['label'] = data['label']\n",
    "    im_scores_gen[i]['suffix'] = data['suffix'][model_ind]    \n",
    "    im_scores_gen[i]['elbo'] = data['elbo'][model_ind]\n",
    "    im_scores_gen[i]['recon'] = data['recons'][model_ind]\n",
    "    im_scores_gen[i]['kld'] = data['klds'][model_ind]\n",
    "    im_scores_gen[i]['klds_per_dim'] = data['klds_per_dim'][model_ind]    \n",
    "    im_scores_gen[i]['epoch'] = data['epoch'][model_ind]\n",
    "    im_scores_gen[i]['im_path'] = '{}/ref_model/progress_{}.png'.format(model_dir, int(data['elbo'][model_ind]))\n",
    "    im_scores_gen[i]['args'] = data['args'][model_ind]\n",
    "    im_scores_gen[i]['beta'] = data['beta']\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Num. models = {len(im_scores_gen)}\\n{im_scores_gen[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for i in im_scores_gen:\n",
    "    #log specific model architechure choices\n",
    "    \n",
    "    color = 'k'\n",
    "    \n",
    "    if im_scores_gen[i]['args']['dataProvider'] == 'RefDataProvider':\n",
    "        im_scores_gen[i]['intensity_norm'] = 0\n",
    "    elif im_scores_gen[i]['args']['dataProvider'] == 'RescaledIntensityRefDataProvider':\n",
    "        im_scores_gen[i]['intensity_norm'] = 1\n",
    "    else:\n",
    "        raise error\n",
    "        \n",
    "    if im_scores_gen[i]['args']['dataProvider'] == 'RescaledIntensityRefDataProvider':\n",
    "        marker = 'p'\n",
    "    else:\n",
    "        marker = '^'\n",
    "        \n",
    "#     im_scores_gen[i]['beta'] = im_scores_gen[i]['args']['kwargs_model']['beta']\n",
    "    im_scores_gen[i]['marker'] = marker\n",
    "    im_scores_gen[i]['color'] = color\n",
    "\n",
    "\n",
    "\n",
    "for i in im_scores_gen:\n",
    "    beta = im_scores_gen[i]['beta']\n",
    "    im_scores_gen[i]['model_arch_str'] = rf\"$ \\beta $ = {beta}\"\n",
    "    \n",
    "df_master = pd.DataFrame.from_dict([im_scores_gen[i] for i in im_scores_gen])    \n",
    "df_master = df_master.sort_values('beta')\n",
    "\n",
    "\n",
    "# for s in df_master['model_arch_str']: print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s in df_master['model_arch_str']: print(s)\n",
    "#print(df_master)\n",
    "df_master[['beta', 'intensity_norm', 'suffix', 'model_dir']].sort_values(['beta', 'intensity_norm'])\n",
    "#df_master.to_csv('~/df_master.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the best-on-validation set models for each $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model = 'asdfasdfasdf'\n",
    "\n",
    "for i, data in enumerate(data_list):\n",
    "    if data['model_dir'] == \"/allen/aics/modeling/gregj/results/integrated_cell/test_cbvae/2019-07-19-09:27:15/\":\n",
    "        best_model = i\n",
    "        break\n",
    "\n",
    "best_model = np.argmax(best_elbo)\n",
    "\n",
    "for data in data_list:\n",
    "# data = data_list[best_model]\n",
    "\n",
    "\n",
    "    ind = 0\n",
    "    save_dir = data['model_dir']\n",
    "\n",
    "    print(\"model_dir = '{}'\".format(data['model_dir']))\n",
    "    print(\"parent_dir = '{}'\".format(parent_dir))\n",
    "    print(\"suffix = '{}'\".format(data['suffix'][ind]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do feature calculation for some subset of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "import importlib as imp\n",
    "import integrated_cell.utils.features\n",
    "imp.reload(integrated_cell.utils)\n",
    "imp.reload(integrated_cell.utils.features)\n",
    "\n",
    "from integrated_cell.utils.features import im2feats\n",
    "from aicsfeature.extractor.common import get_shape_features, get_intensity_features, get_skeleton_features\n",
    "\n",
    "from scipy import ndimage\n",
    "from skimage import data as skdata\n",
    "#from skimage.filters import threshold_otsu, threshold_local, try_all_threshold\n",
    "from skimage import filters, morphology, measure, color\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reshape_subplots(fig, ax):\n",
    "    num_subplots = len(ax.flatten())\n",
    "    \n",
    "    gs = matplotlib.gridspec.GridSpec(1, num_subplots)\n",
    "\n",
    "    for index in range(num_subplots):\n",
    "        ax[index].set_position(gs[index].get_position(fig))\n",
    "        \n",
    "    return\n",
    "\n",
    "def process_binmask(binmask):\n",
    "    # Generate label for unconnected objects in the binary mask\n",
    "    labels = measure.label(binmask)\n",
    "    assert(labels.max() != 0)  # There should be at least 1 CC\n",
    "    \n",
    "    # Use regionprops to extract the properties of each object\n",
    "    region_props = measure.regionprops(labels)\n",
    "    \n",
    "    # Sort the areas of all objecs and get the label of the\n",
    "    # largest one\n",
    "    areas = [(obj_prop['label'], obj_prop['area']) for obj_prop in region_props]\n",
    "    sorted_areas = sorted(areas, key=lambda x:x[1], reverse=True)\n",
    "    max_label = sorted_areas[0][0]  # Get the label of the largest object\n",
    "    \n",
    "    # Mask out all objects except for the largest one\n",
    "    single_mask = np.where(labels == max_label, 1, 0)\n",
    "    \n",
    "    # Fill in any holes in the largest object\n",
    "    filled_mask = ndimage.binary_fill_holes(single_mask)\n",
    "    \n",
    "    return filled_mask    \n",
    "    \n",
    "def get_binmask(im, method='local_mean', local_block_size=199, debug=False):\n",
    "    # methods = {'gt_zero', otsu', 'local_gaussian', 'local_mean', 'local_median', 'li', 'mean', 'all'}\n",
    "    \n",
    "    if (method == 'gt_zero' or method == 'all'):\n",
    "        # Estimate the optimal threshold\n",
    "        flt_threshold_gt_zero = 0\n",
    "        \n",
    "        # Threshold the image to get the binary mask\n",
    "        im_binmask_gt_zero = im > flt_threshold_gt_zero\n",
    "        \n",
    "        # Process the binary mask (e.g. keep the largest object, fill holes)\n",
    "        im_binmask_gt_zero = process_binmask(im_binmask_gt_zero)\n",
    "        im_binmask = im_binmask_gt_zero\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'otsu' or method == 'all'):\n",
    "        # Estimate the optimal threshold\n",
    "        flt_threshold_otsu = filters.threshold_otsu(im)\n",
    "        \n",
    "        # Threshold the image to get the binary mask\n",
    "        im_binmask_otsu = im > flt_threshold_otsu\n",
    "        \n",
    "        # Process the binary mask (e.g. keep the largest object, fill holes)\n",
    "        im_binmask_otsu = process_binmask(im_binmask_otsu)\n",
    "        im_binmask = im_binmask_otsu\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'local_gaussian' or method == 'all'):\n",
    "        flt_threshold_local_gaussian = filters.threshold_local(im, local_block_size, 'gaussian')\n",
    "        im_binmask_local_gaussian = im > flt_threshold_local_gaussian\n",
    "        im_binmask_local_gaussian = process_binmask(im_binmask_local_gaussian)\n",
    "        im_binmask = im_binmask_local_gaussian\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'local_mean' or method == 'all'):\n",
    "        flt_threshold_local_mean = filters.threshold_local(im, local_block_size, 'mean')\n",
    "        im_binmask_local_mean = im > flt_threshold_local_mean\n",
    "        im_binmask_local_mean = process_binmask(im_binmask_local_mean)\n",
    "        im_binmask = im_binmask_local_mean\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'local_median' or method == 'all'):\n",
    "        flt_threshold_local_median = filters.threshold_local(im, local_block_size, 'median')\n",
    "        im_binmask_local_median = im > flt_threshold_local_median\n",
    "        im_binmask_local_median = process_binmask(im_binmask_local_median)\n",
    "        im_binmask = im_binmask_local_median\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'li' or method == 'all'):\n",
    "        flt_threshold_li = filters.threshold_li(im)\n",
    "        im_binmask_li = im > flt_threshold_li\n",
    "        im_binmask_li = process_binmask(im_binmask_li)\n",
    "        im_binmask = im_binmask_li\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'mean' or method == 'all'):\n",
    "        flt_threshold_mean = filters.threshold_mean(im)\n",
    "        im_binmask_mean = im > flt_threshold_mean\n",
    "        im_binmask_mean = process_binmask(im_binmask_mean)\n",
    "        im_binmask = im_binmask_mean\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'all'):\n",
    "        return im_binmask_gt_zero, im_binmask_otsu, im_binmask_local_gaussian, im_binmask_local_mean, im_binmask_local_median, im_binmask_li, im_binmask_mean\n",
    "    else:\n",
    "        return im_binmask\n",
    "\n",
    "def save_feats(im, save_path, seg_method='local_mean', mask_intensity_features=True, save_imgs=False, figsize_hist=(30, 4), figsize_cells=(30, 4), dpi=100, debug=False):\n",
    "    \n",
    "    assert im.shape[0] == 1 \n",
    "    \n",
    "    im_tmp = im[0].cpu().numpy()\n",
    "    if debug:\n",
    "        print(f'im_tmp.shape = {im_tmp.shape}')\n",
    "        #imshow2ch(im_tmp)\n",
    "\n",
    "    im_tmp = np.expand_dims(im_tmp, 3)\n",
    "    if debug: print(f'im_tmp.shape = {im_tmp.shape}')\n",
    "\n",
    "    im_struct = np.copy(im_tmp)\n",
    "    if debug: print(f'im_struct.shape = {im_struct.shape}')\n",
    "    \n",
    "    for i in range(im_struct.shape[0]):\n",
    "        if debug: print(f'np.max(im_struct[{i}]) = {np.max(im_struct[i])}')\n",
    "        im_struct[i] = (im_struct[i] / np.max(im_struct[i]))*255\n",
    "        \n",
    "    im_struct = im_struct.astype('uint8')\n",
    "    if debug: print(f'im_struct.shape = {im_struct.shape}')\n",
    "    \n",
    "    #if debug:\n",
    "    #    imshow2ch(im_struct[:, :, :, 0]>0)\n",
    "    #    imshow2ch(im_struct[:, :, :, 0])\n",
    "        \n",
    "    feats = {}\n",
    "    \n",
    "    if (seg_method == 'all'):\n",
    "        cell_binmask_gt_zero, cell_binmask_otsu, cell_binmask_local_gaussian, cell_binmask_local_mean, cell_binmask_local_median, cell_binmask_li, cell_binmask_mean = get_binmask(im_struct[0, :, :, 0], method=seg_method, debug=debug)\n",
    "        nuc_binmask_gt_zero, nuc_binmask_otsu, nuc_binmask_local_gaussian, nuc_binmask_local_mean, nuc_binmask_local_median, nuc_binmask_li, nuc_binmask_mean = get_binmask(im_struct[1, :, :, 0], method=seg_method, debug=debug)\n",
    "        \n",
    "        # Use local_mean method for feature extraction\n",
    "        cell_binmask = cell_binmask_local_mean\n",
    "        nuc_binmask = nuc_binmask_local_mean\n",
    "        \n",
    "    else:\n",
    "        cell_binmask = get_binmask(im_struct[0, :, :, 0], method=seg_method, debug=debug)\n",
    "        nuc_binmask = get_binmask(im_struct[1, :, :, 0], method=seg_method, debug=debug)\n",
    "    \n",
    "    #feats['dna_shape'] = get_shape_features(seg=im_struct[1]>0)\n",
    "    feats['dna_shape'] = get_shape_features(seg=nuc_binmask[:, :, np.newaxis])\n",
    "    if debug: print(f\"dna_shape = {feats['dna_shape']}\")\n",
    "        \n",
    "    if (mask_intensity_features):\n",
    "        feats['dna_inten'] = get_intensity_features(img=np.where(nuc_binmask[:, :, np.newaxis] > 0, im_struct[1], 0))  # Apply binary mask to image\n",
    "    else:\n",
    "        feats['dna_inten'] = get_intensity_features(img=im_struct[1])\n",
    "    if debug: print(f\"dna_inten = {feats['dna_inten']}\")\n",
    "    \n",
    "    #try:\n",
    "    #    feats['dna_skeleton'] = get_skeleton_features(seg=im_struct[1])\n",
    "    #except:\n",
    "    #    feats['cell_skeleton'] = {}\n",
    "    #if debug: print(f\"dna_skeleton = {feats['dna_skeleton']}\")\n",
    "\n",
    "    #feats['cell_shape'] = get_shape_features(seg=im_struct[0]>0)\n",
    "    feats['cell_shape'] = get_shape_features(seg=cell_binmask[:, :, np.newaxis])\n",
    "    if debug: print(f\"cell_shape = {feats['cell_shape']}\")\n",
    "        \n",
    "    if (mask_intensity_features):\n",
    "        feats['cell_inten'] = get_intensity_features(img=np.where(cell_binmask[:, :, np.newaxis] > 0, im_struct[0], 0))  # Apply binary mask to image\n",
    "    else:\n",
    "        feats['cell_inten'] = get_intensity_features(img=im_struct[0])\n",
    "    if debug: print(f\"cell_inten = {feats['cell_inten']}\")\n",
    "    \n",
    "    #try:\n",
    "    #    feats['cell_skeleton'] = get_skeleton_features(seg=im_struct[0])\n",
    "    #except:\n",
    "    #    feats['cell_skeleton'] = {}\n",
    "    #if debug: print(f\"cell_skeleton = {feats['cell_skeleton']}\")\n",
    "#     feats = im2feats(im_struct[0], im_struct[1], im_struct, extra_features=[\"io_intensity\", \"bright_spots\", \"intensity\", \"skeleton\"])\n",
    "    \n",
    "    if (save_imgs):\n",
    "        # Show and save histograms of input images (before and after normalization)\n",
    "        objFig_hist, objAxes_hist = plt.subplots(1, 4, figsize=figsize_hist)\n",
    "\n",
    "        objAxes_hist[0].hist(im_tmp[0, :, :, 0].flatten())\n",
    "        objAxes_hist[0].set_yscale('log')\n",
    "        objAxes_hist[0].set_title('cell_img')\n",
    "\n",
    "        objAxes_hist[1].hist(im_tmp[1, :, :, 0].flatten())\n",
    "        objAxes_hist[1].set_title('nuc_img')\n",
    "        objAxes_hist[1].set_yscale('log')\n",
    "\n",
    "        objAxes_hist[2].hist(im_struct[0, :, :, 0].flatten())\n",
    "        objAxes_hist[2].set_yscale('log')\n",
    "        objAxes_hist[2].set_title('cell_img_norm')\n",
    "\n",
    "        objAxes_hist[3].hist(im_struct[1, :, :, 0].flatten())\n",
    "        objAxes_hist[3].set_title('nuc_img_norm')\n",
    "        objAxes_hist[3].set_yscale('log')\n",
    "\n",
    "        plt.savefig(save_path.replace('.pkl', '_hist.png'), bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "        \n",
    "        if (seg_method == 'all'):\n",
    "            # Show and save input images (before and after segmentation)\n",
    "            objFig, objAxes = plt.subplots(1, 16, figsize=figsize_cells)\n",
    "\n",
    "            objAxes[0].imshow(im_tmp[0, :, :, 0], cmap='gray')\n",
    "            objAxes[0].set_title('cell_img')\n",
    "\n",
    "            objAxes[1].imshow(im_tmp[1, :, :, 0], cmap='gray')\n",
    "            objAxes[1].set_title('nuc_img')\n",
    "\n",
    "            #objAxes[2].imshow(im_struct[0, :, :, 0], cmap='gray')\n",
    "            #objAxes[2].set_title('cell_img_norm')\n",
    "\n",
    "            #objAxes[3].imshow(im_struct[1, :, :, 0], cmap='gray')\n",
    "            #objAxes[3].set_title('nuc_img_norm')\n",
    "\n",
    "            #objAxes[2].imshow(im_struct[0, :, :, 0] > 0, cmap='gray')\n",
    "            objAxes[2].imshow(cell_binmask_gt_zero, cmap='gray')\n",
    "            objAxes[2].set_title('> 0')\n",
    "\n",
    "            #objAxes[3].imshow(im_struct[1, :, :, 0] > 0, cmap='gray')\n",
    "            objAxes[3].imshow(nuc_binmask_gt_zero, cmap='gray')\n",
    "            objAxes[3].set_title('> 0')\n",
    "\n",
    "            objAxes[4].imshow(cell_binmask_otsu, cmap='gray')\n",
    "            objAxes[4].set_title('Otsu')\n",
    "\n",
    "            objAxes[5].imshow(nuc_binmask_otsu, cmap='gray')\n",
    "            objAxes[5].set_title('Otsu')\n",
    "\n",
    "            objAxes[6].imshow(cell_binmask_local_gaussian, cmap='gray')  # Good\n",
    "            objAxes[6].set_title('Local (Gaussian)')\n",
    "\n",
    "            objAxes[7].imshow(nuc_binmask_local_gaussian, cmap='gray')\n",
    "            objAxes[7].set_title('Local (Gaussian)')\n",
    "\n",
    "            objAxes[8].imshow(cell_binmask_local_mean, cmap='gray')  # Good\n",
    "            objAxes[8].set_title('Local (Mean)')\n",
    "\n",
    "            objAxes[9].imshow(nuc_binmask_local_mean, cmap='gray')\n",
    "            objAxes[9].set_title('Local (Mean)')\n",
    "\n",
    "            objAxes[10].imshow(cell_binmask_local_median, cmap='gray')\n",
    "            objAxes[10].set_title('Local (Median)')\n",
    "\n",
    "            objAxes[11].imshow(nuc_binmask_local_median, cmap='gray')\n",
    "            objAxes[11].set_title('Local (Median)')\n",
    "\n",
    "            objAxes[12].imshow(cell_binmask_li, cmap='gray')\n",
    "            objAxes[12].set_title('Li')\n",
    "\n",
    "            objAxes[13].imshow(nuc_binmask_li, cmap='gray')\n",
    "            objAxes[13].set_title('Li')\n",
    "\n",
    "            objAxes[14].imshow(cell_binmask_mean, cmap='gray')\n",
    "            objAxes[14].set_title('Mean')\n",
    "\n",
    "            objAxes[15].imshow(nuc_binmask_mean, cmap='gray')\n",
    "            objAxes[15].set_title('Mean')\n",
    "            \n",
    "        else:\n",
    "            # Show and save input images (before and after segmentation)\n",
    "            objFig, objAxes = plt.subplots(1, 4, figsize=figsize_cells)\n",
    "\n",
    "            objAxes[0].imshow(im_tmp[0, :, :, 0], cmap='gray')\n",
    "            objAxes[0].set_title('cell_img')\n",
    "\n",
    "            objAxes[1].imshow(im_tmp[1, :, :, 0], cmap='gray')\n",
    "            objAxes[1].set_title('nuc_img')\n",
    "\n",
    "            #objAxes[2].imshow(im_struct[0, :, :, 0], cmap='gray')\n",
    "            #objAxes[2].set_title('cell_img_norm')\n",
    "\n",
    "            #objAxes[3].imshow(im_struct[1, :, :, 0], cmap='gray')\n",
    "            #objAxes[3].set_title('nuc_img_norm')\n",
    "\n",
    "            #objAxes[2].imshow(im_struct[0, :, :, 0] > 0, cmap='gray')\n",
    "            objAxes[2].imshow(cell_binmask, cmap='gray')\n",
    "            objAxes[2].set_title(seg_method)\n",
    "\n",
    "            #objAxes[3].imshow(im_struct[1, :, :, 0] > 0, cmap='gray')\n",
    "            objAxes[3].imshow(nuc_binmask, cmap='gray')\n",
    "            objAxes[3].set_title(seg_method)            \n",
    "\n",
    "        for objAxis in objAxes:\n",
    "            objAxis.axis('off')\n",
    "\n",
    "        plt.savefig(save_path.replace('.pkl', '_img.png'), bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\n",
    "        # NOTE: Li and mean thresholds give good results\n",
    "        #fig, ax = filters.try_all_threshold(im_struct[0, :, :, 0], figsize=(20, 4), verbose=False)\n",
    "        #reshape_subplots(fig, ax)\n",
    "        #plt.savefig(save_path.replace('.pkl', '_cellseg.png'), bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\n",
    "        #fig, ax = filters.try_all_threshold(im_struct[1, :, :, 0], figsize=(20, 4), verbose=False)\n",
    "        #reshape_subplots(fig, ax)\n",
    "        #plt.savefig(save_path.replace('.pkl', '_nucseg.png'), bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\n",
    "        if (not fnIsBatchMode()):\n",
    "            plt.show()\n",
    "\n",
    "        plt.close('all')\n",
    "    \n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(feats, f)\n",
    "    \n",
    "    return\n",
    "\n",
    "import re\n",
    "\n",
    "def load_feats(save_paths):\n",
    "    feats = list()\n",
    "    for save_path in save_paths:\n",
    "        with open(save_path, 'rb') as f:\n",
    "            feat_tmp = pickle.load(f)\n",
    "            \n",
    "        feat = {}\n",
    "        \n",
    "        # Extract the cell_idx from the filename\n",
    "        _, basename, _ = fnSplitFullFilename(save_path)\n",
    "        result = re.match('feat_(\\d+)', basename)\n",
    "        cell_idx = result.groups()[0]\n",
    "        feat['cell_idx'] = cell_idx\n",
    "        \n",
    "        for i in feat_tmp:\n",
    "            for j in feat_tmp[i]:\n",
    "                feat[\"{}_{}\".format(i,j)] = feat_tmp[i][j]            \n",
    "            \n",
    "        feats.append(feat)\n",
    "\n",
    "    feats = pd.DataFrame.from_dict(feats)\n",
    "        \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import integrated_cell.metrics.embeddings_reference as get_embeddings_reference\n",
    "\n",
    "def fnLoadUnsortedEmbeddings(argUnsortedEmbeddingsPath = './dctUnsortedEmbeddings.pth', argEncoder = None, argDecoder = None, argDataProvider = None, argArgs = None, argReGen = False):\n",
    "    # See if the embedding file already exists. If so, load it\n",
    "    if (os.path.exists(argUnsortedEmbeddingsPath) and not(argReGen)):\n",
    "        print('Loading unsorted embeddings file from: {}'.format(argUnsortedEmbeddingsPath))\n",
    "        embeddings_ref_untreated_ref_mu = torch.load(argUnsortedEmbeddingsPath)\n",
    "        \n",
    "    # If the embedding file does not exist, generate one\n",
    "    else:\n",
    "        print('Generating unsorted embeddings file at: {}'.format(argUnsortedEmbeddingsPath))\n",
    "        \n",
    "        ref_enc = argEncoder\n",
    "        ref_dec = argDecoder\n",
    "        dp_ref  = argDataProvider\n",
    "\n",
    "        recon_loss = utils.load_losses(argArgs)['crit_recon']\n",
    "        batch_size = dp_ref.batch_size\n",
    "        \n",
    "        ### get embeddings for all cells and save to dict\n",
    "        embeddings_ref_untreated_val = get_embeddings_reference.get_latent_embeddings(\n",
    "            ref_enc,\n",
    "            ref_dec,\n",
    "            dp_ref,\n",
    "            recon_loss,\n",
    "            batch_size=batch_size,\n",
    "            modes=['validate'],\n",
    "        )\n",
    "        embeddings_ref_untreated_test = get_embeddings_reference.get_latent_embeddings(\n",
    "            ref_enc,\n",
    "            ref_dec,\n",
    "            dp_ref,\n",
    "            recon_loss,\n",
    "            batch_size=batch_size,\n",
    "            modes=['test'],\n",
    "        )\n",
    "        embeddings_ref_untreated_train = get_embeddings_reference.get_latent_embeddings(\n",
    "            ref_enc,\n",
    "            ref_dec,\n",
    "            dp_ref,\n",
    "            recon_loss,\n",
    "            batch_size=batch_size,\n",
    "            modes=['train'],\n",
    "        )\n",
    "        embeddings_ref_untreated_ref_mu = {\n",
    "            \"train\": embeddings_ref_untreated_train[\"train\"][\"ref\"][\"mu\"],\n",
    "            \"test\": embeddings_ref_untreated_test[\"test\"][\"ref\"][\"mu\"],\n",
    "            \"validate\": embeddings_ref_untreated_val[\"validate\"][\"ref\"][\"mu\"],\n",
    "        }\n",
    "\n",
    "        torch.save(embeddings_ref_untreated_ref_mu, argUnsortedEmbeddingsPath)\n",
    "        \n",
    "    return embeddings_ref_untreated_ref_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "strEmbeddingsParentPath = '/allen/aics/modeling/caleb/data/'\n",
    "\n",
    "def imshow2ch(im):\n",
    "    objFig, (objAx1, objAx2) = plt.subplots(1, 2, figsize=(8, 5))\n",
    "    objAx1.imshow(im[0, :, :], cmap='gray')\n",
    "    objAx2.imshow(im[1, :, :], cmap='gray')\n",
    "    \n",
    "def fnGenUnsortedEmbeddingsPath(argEmbeddingsParentPath, argModelDir, argRefSuffix, argNumPathLevels=2):\n",
    "    strModelName = '_'.join(re.sub('\\/\\/+', '/', argModelDir.strip('/').replace(':', '-')).split('/')[-argNumPathLevels:])  # Replace multiple slashes with single slash and split the path\n",
    "    return argEmbeddingsParentPath + 'dctUnsortedEmbeddings_2DModel_' + strModelName + argRefSuffix + '.pth'\n",
    "    \n",
    "def fnGenRandomZ(argEmbeddingsParentPath, argModelDir, argRefSuffix, argBatchSize=1, argMode='train'):\n",
    "    strEmbeddingsFullFilename = fnGenUnsortedEmbeddingsPath(argEmbeddingsParentPath, argModelDir, argRefSuffix)\n",
    "    dctUnsortedEmbeddings = fnLoadUnsortedEmbeddings(strEmbeddingsFullFilename)\n",
    "    embeddings_unfiltered = dctUnsortedEmbeddings[argMode]\n",
    "    print(f'embeddings_unfiltered.shape = {embeddings_unfiltered.shape}')\n",
    "    \n",
    "    arrEmbeddingsMean = torch.mean(embeddings_unfiltered, dim=0, keepdim=True)\n",
    "    arrEmbeddingsStdev = torch.std(embeddings_unfiltered, dim=0, keepdim=True)\n",
    "    #print(f'{arrEmbeddingsMean.shape}, {arrEmbeddingsStdev.shape}')\n",
    "    \n",
    "    arrRandomZ = torch.normal(mean=arrEmbeddingsMean.repeat(argBatchSize, 1), std=arrEmbeddingsStdev.repeat(argBatchSize, 1))\n",
    "    \n",
    "    return arrRandomZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# 5 cells with save_imgs (all seg methods) = 1 hr\n",
    "# 20 cells with save_imgs (1 seg method) = 12 mins\n",
    "# 100 cells, no save_imgs, 3 betas = 7 mins\n",
    "# 100 cells, no save_imgs, 4 betas = 9 mins\n",
    "intNumCells = 20  # Set to < 0 to select the entire test set\n",
    "\n",
    "# methods = {'gt_zero', otsu', 'local_gaussian', 'local_mean', 'local_median', 'li', 'mean', 'all'}\n",
    "seg_method_real = 'gt_zero'  # Should be gt_zero for real cells\n",
    "seg_method_gen = 'local_mean'  # Should be local_mean for generated cells\n",
    "\n",
    "# Whether to mask the intensity images before feature extraction\n",
    "# TODO: Test whether this makes a difference for the generated cells\n",
    "mask_intensity_features_real = True\n",
    "mask_intensity_features_gen = True\n",
    "\n",
    "save_imgs = True  # Whether to save the binary masks\n",
    "figsize_hist = (16, 2)  # Large = (30, 4), small = (16, 2)\n",
    "debug = False\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "# All = (30, 4), single = (10, 4)\n",
    "figsize_cells_real = (30, 4) if seg_method_real == 'all' else (10, 4)\n",
    "figsize_cells_gen = (30, 4) if seg_method_gen == 'all' else (10, 4)\n",
    "\n",
    "datStart = fnNow()\n",
    "print(f'Started on {fnGetDatetime(datStart)}')\n",
    "print()\n",
    "\n",
    "#feats_parent_dir = \"{}/feats/\".format(results_dir)\n",
    "feats_parent_dir = \"{}/feats_caleb/\".format(results_dir)\n",
    "print(f'feats_parent_dir = {feats_parent_dir}')\n",
    "\n",
    "all_feats_save_path = \"{}/all_feats.pkl\".format(feats_parent_dir)\n",
    "print(f'all_feats_save_path = {all_feats_save_path}')\n",
    "\n",
    "intensity_norms = np.unique(df_master['intensity_norm'])\n",
    "\n",
    "feature_path_dict = {}\n",
    "#there are 2 normalization methods\n",
    "for intensity_norm in intensity_norms:\n",
    "    print(f'  intensity_norm = {intensity_norm}')\n",
    "    \n",
    "    #get the dataframe for this normalization method\n",
    "    df_norm = df_master[df_master['intensity_norm'] == intensity_norm]\n",
    "    \n",
    "    #get the parent directory for saving this normalization method    \n",
    "    save_norm_parent = \"{}/norm_{}\".format(feats_parent_dir, intensity_norm)\n",
    "    print(f'  save_norm_parent = {save_norm_parent}')\n",
    "    if not os.path.exists(save_norm_parent):\n",
    "        os.makedirs(save_norm_parent)\n",
    "        \n",
    "    save_norm_feats = \"{}/feats_test\".format(save_norm_parent)\n",
    "    print(f'  save_norm_feats = {save_norm_feats}')\n",
    "    if not os.path.exists(save_norm_feats):\n",
    "        os.makedirs(save_norm_feats)\n",
    "        \n",
    "    #get a data provider for this normalization methods\n",
    "    networks, dp, args = utils.load_network_from_dir(df_norm['model_dir'].iloc[0], parent_dir, suffix=df_norm['suffix'].iloc[0])\n",
    "    \n",
    "    enc = networks['enc'].cuda()\n",
    "    \n",
    "    x = dp.get_sample()\n",
    "    print(f'  x.shape = {x.shape}')\n",
    "    \n",
    "    z_tmp = enc(x.cuda())[0]\n",
    "    z_tmp = z_tmp[[0]]\n",
    "    print(f'  z_tmp.shape = {z_tmp.shape}')\n",
    "    \n",
    "    n_latent = z_tmp.shape[1]\n",
    "    \n",
    "    if (intNumCells <= 0):\n",
    "        intNumCells = dp.get_n_dat('test')\n",
    "        \n",
    "    #cell_idx = range(dp.get_n_dat('test'))\n",
    "    cell_idx = range(intNumCells)\n",
    "    \n",
    "    #n_dat = dp.get_n_dat('test')\n",
    "    n_dat = len(cell_idx)\n",
    "    \n",
    "\n",
    "    #save_real_feats_paths = ['{}/feat_{}.pkl'.format(save_norm_feats, i) for i in range(n_dat)]\n",
    "    save_real_feats_paths = ['{}/feat_{}.pkl'.format(save_norm_feats, i) for i in cell_idx]\n",
    "    \n",
    "    # Loop through all the real images (test set) and save them\n",
    "    for i, save_real_feat_path in tqdm(enumerate(save_real_feats_paths)):\n",
    "        if not os.path.exists(save_real_feat_path):\n",
    "            print(f'    i = {i}, cell_idx = {cell_idx[i]}, save_real_feat_path = {save_real_feat_path}')\n",
    "            #im = dp.get_sample('test', [i])   \n",
    "            im = dp.get_sample('test', [cell_idx[i]])\n",
    "            save_feats(im, \n",
    "                       save_real_feat_path, \n",
    "                       seg_method=seg_method_real, \n",
    "                       mask_intensity_features=mask_intensity_features_real, \n",
    "                       save_imgs=save_imgs, \n",
    "                       figsize_hist=figsize_hist, \n",
    "                       figsize_cells=figsize_cells_real, \n",
    "                       debug=debug)\n",
    "\n",
    "    feature_path_dict[intensity_norm] = {}\n",
    "    feature_path_dict[intensity_norm]['real'] = load_feats(save_real_feats_paths)\n",
    "    feature_path_dict[intensity_norm]['gen'] = {}\n",
    "    \n",
    "    # now loop through all the models under this normalization method, saving generated images and features\n",
    "    for i in range(df_norm.shape[0]):\n",
    "        print(f'    i_df_norm = {i}')\n",
    "        \n",
    "        # ***BUG?: I think we should be using df_norm below, and not df_master\n",
    "        #save_feats_dir = '{}/{}'.format(save_norm_parent, df_master['label'].iloc[i])\n",
    "        save_feats_dir = '{}/{}'.format(save_norm_parent, df_norm['label'].iloc[i])\n",
    "        print(f'    save_feats_dir = {save_feats_dir}')\n",
    "        \n",
    "        if not os.path.exists(save_feats_dir):\n",
    "            os.makedirs(save_feats_dir)\n",
    "        \n",
    "        #load the network\n",
    "        network_loaded = False\n",
    "        \n",
    "        # ***BUG?: Will this line always be using the network\n",
    "        #          loaded in the last loop except for the first\n",
    "        #          loop???\n",
    "        #dec = networks['dec'].cuda()\n",
    "        \n",
    "        strModelDir = df_norm['model_dir'].iloc[i]\n",
    "        strRefSuffix = df_norm['suffix'].iloc[i]\n",
    "        \n",
    "        beta = df_norm['beta'].iloc[i]\n",
    "        print(f'    beta = {beta}')\n",
    "        \n",
    "        gen_real_path = f'{save_feats_dir}/real'\n",
    "        gen_kld_path = f'{save_feats_dir}/kld'\n",
    "        gen_norm_path = f'{save_feats_dir}/norm'\n",
    "        \n",
    "        os.makedirs(gen_real_path, exist_ok=True)\n",
    "        os.makedirs(gen_kld_path, exist_ok=True)\n",
    "        os.makedirs(gen_norm_path, exist_ok=True)\n",
    "        \n",
    "        save_gen_feats_paths_real = ['{}/feat_{}.pkl'.format(gen_real_path, i) for i in range(n_dat)]\n",
    "        save_gen_feats_paths_kld = ['{}/feat_{}.pkl'.format(gen_kld_path, i) for i in range(n_dat)]\n",
    "        save_gen_feats_paths_norm = ['{}/feat_{}.pkl'.format(gen_norm_path, i) for i in range(n_dat)]\n",
    "        \n",
    "        save_gen_feats_paths_dct = {\n",
    "            'real': save_gen_feats_paths_real, \n",
    "            'rnd_kld': save_gen_feats_paths_kld, \n",
    "            'rnd_norm': save_gen_feats_paths_norm, \n",
    "        }\n",
    "        \n",
    "        # TODO:\n",
    "        #   - test embeddings should be loaded and available here, and then integrated with the feature tables\n",
    "        #   - z_tmps generated should be saved in save_feats_dir\n",
    "        #   - images should also be saved in save_feats here\n",
    "        \n",
    "        # (1) Real cell features\n",
    "        # (2) Gen cell features (from real cells) + latent space embeddings\n",
    "        # (3) Gen cell features (random from unit gaussian) + latent space embeddings\n",
    "        # (4) Gen cell features (ramdom from latent space dims) + latent space embeddings\n",
    "\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        strEmbeddingsFullFilename = fnGenUnsortedEmbeddingsPath(strEmbeddingsParentPath, strModelDir, strRefSuffix)\n",
    "        dctUnsortedEmbeddings = fnLoadUnsortedEmbeddings(strEmbeddingsFullFilename)\n",
    "        z_test = dctUnsortedEmbeddings['test'][cell_idx, :]\n",
    "        print(f'    z_test.shape = {z_test.shape}')\n",
    "        \n",
    "        z_rnd_kld = fnGenRandomZ(strEmbeddingsParentPath, strModelDir, strRefSuffix, argBatchSize=n_dat)\n",
    "        print(f'    z_rnd_kld.shape = {z_rnd_kld.shape}')\n",
    "        \n",
    "        z_rnd_norm = z_tmp[0, :].repeat(n_dat, 1)\n",
    "        z_rnd_norm.normal_()\n",
    "        print(f'    z_rnd_norm.shape = {z_rnd_norm.shape}')\n",
    "        \n",
    "        # Save embeddings (test set or random) to embeddings_z_test/rnd_kld/rnd_norm.pkl\n",
    "        with open(f'{gen_real_path}/embeddings.pkl', \"wb\") as f:\n",
    "            pickle.dump(z_test, f)\n",
    "        print(f'    Saving embeddings (real) to {gen_real_path}/embeddings.pkl')\n",
    "        \n",
    "        with open(f'{gen_kld_path}/embeddings.pkl', \"wb\") as f:\n",
    "            pickle.dump(z_rnd_kld, f)\n",
    "        print(f'    Saving embeddings (rnd_kld) to {gen_kld_path}/embeddings.pkl')\n",
    "        \n",
    "        with open(f'{gen_norm_path}/embeddings.pkl', \"wb\") as f:\n",
    "            pickle.dump(z_rnd_norm, f)\n",
    "        print(f'    Saving embeddings (rnd_norm) to {gen_norm_path}/embeddings.pkl')\n",
    "        \n",
    "        feature_path_dict[intensity_norm]['gen'][beta] = {}\n",
    "        \n",
    "        for key in save_gen_feats_paths_dct.keys():\n",
    "            \n",
    "            save_gen_feats_paths = save_gen_feats_paths_dct[key]\n",
    "            \n",
    "            if (key == 'real'):\n",
    "                z_tmp = z_test\n",
    "                \n",
    "            elif (key == 'rnd_kld'):\n",
    "                z_tmp = z_rnd_kld\n",
    "                \n",
    "            else:\n",
    "                z_tmp = z_rnd_norm\n",
    "            \n",
    "            for j, save_path in tqdm(enumerate(save_gen_feats_paths)):\n",
    "\n",
    "                if not os.path.exists(save_path):\n",
    "                    print(f'      j = {j}, save_path = {save_path}')\n",
    "\n",
    "                    if not network_loaded:\n",
    "                        networks, dp, args = utils.load_network_from_dir(df_norm['model_dir'].iloc[i], parent_dir, suffix=df_norm['suffix'].iloc[i])\n",
    "                        dec = networks['dec'].cuda()  # ***BUG?: How come we load the network without assigning the enc or dec even though we use it in the next few lines?\n",
    "                        network_loaded = True\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # Randomly sample from a normal distribution (default mean = 0, stdev = 1)\n",
    "                        # Does this make sense given that the latent space is not necessarily a unit\n",
    "                        # Gaussian distribution?\n",
    "                        #im = dec(z_tmp.normal_())\n",
    "                        im = dec(z_tmp[j, :][np.newaxis, :].cuda())\n",
    "                        print(f'      z_tmp.shape = {z_tmp.shape}\\nim.shape = {im.shape}')\n",
    "\n",
    "                    save_feats(im, \n",
    "                               save_path, \n",
    "                               seg_method=seg_method_gen, \n",
    "                               mask_intensity_features=mask_intensity_features_gen, \n",
    "                               save_imgs=save_imgs, \n",
    "                               figsize_hist=figsize_hist, \n",
    "                               figsize_cells=figsize_cells_gen, \n",
    "                               debug=debug)\n",
    "\n",
    "            #beta = df_norm['beta'].iloc[i]\n",
    "            #print(f'    beta = {beta}')\n",
    "\n",
    "            #feature_path_dict[intensity_norm]['gen'][beta][key] = load_feats(save_gen_feats_paths)\n",
    "            \n",
    "            feature_path_dict[intensity_norm]['gen'][beta][key] = {}\n",
    "            feature_path_dict[intensity_norm]['gen'][beta][key]['embeddings'] = z_tmp.cpu().detach().numpy()\n",
    "            feature_path_dict[intensity_norm]['gen'][beta][key]['features'] = load_feats(save_gen_feats_paths)\n",
    "            \n",
    "with open(all_feats_save_path, \"wb\") as f:\n",
    "    pickle.dump(feature_path_dict, f)\n",
    "\n",
    "datEnd = fnNow()\n",
    "print()\n",
    "print(f'Ended on {fnGetDatetime(datEnd)}')\n",
    "\n",
    "datDuration = datEnd - datStart\n",
    "print(f'datDuration = {datDuration}')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
