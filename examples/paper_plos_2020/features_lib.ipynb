{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eccd61-eb16-4724-9a7a-00ea1c3b1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect whether the script is running in Jupyter or in\n",
    "# batch mode\n",
    "def fnIsBatchMode(argDebug = False):\n",
    "    try:\n",
    "        get_ipython\n",
    "        \n",
    "    except:\n",
    "        blnBatchMode = True\n",
    "        if (argDebug): print('Batch mode detected')\n",
    "        \n",
    "    else:\n",
    "        blnBatchMode = False\n",
    "        print('Interactive mode detected')\n",
    "        \n",
    "    return blnBatchMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb4458-3e74-496d-9958-f364005c2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "strTimeZone = 'America/Los_Angeles'\n",
    "\n",
    "# Returns the current datetime\n",
    "def fnNow():\n",
    "    return datetime.datetime.now(timezone(strTimeZone))\n",
    "\n",
    "# Returns a timestamp (for use in filenames) in the format:\n",
    "# YYYY/MM/DD-HH:MM:SS\n",
    "#def fnGenTimestamp(argDatetime = fnNow()):  # TODO: For some reason this doesn't work\n",
    "def fnGenTimestamp(argDatetime = None):\n",
    "    if (argDatetime == None):\n",
    "        argDatetime = fnNow()\n",
    "    return argDatetime.strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# Returns a date and time in human-readable format\n",
    "#def fnGetDatetime(argDatetime = fnNow()):  # TODO: For some reason this doesn't work\n",
    "def fnGetDatetime(argDatetime = None):\n",
    "    if (argDatetime == None):\n",
    "        argDatetime = fnNow()\n",
    "    return argDatetime.strftime('%m/%d/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d13906-7959-406d-9559-118ab3323f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Checks to see if a specified directory exists. If not, create a new one\n",
    "def fnOSMakeDir(argPath):\n",
    "    if (not os.path.exists(argPath)):\n",
    "        print('The specified directory does not exist. Creating a new one: {}'.format(argPath))\n",
    "        os.mkdir(argPath)\n",
    "        \n",
    "def fnSplitFullFilename(argFullFilename, argDebug = False):\n",
    "    strPath, strFilename = os.path.split(argFullFilename)\n",
    "    strBasename, strExt = os.path.splitext(strFilename)\n",
    "    \n",
    "    return strPath, strBasename, strExt\n",
    "\n",
    "# Returns a list of full filenames recursively from a parent directory,\n",
    "# sorted and filtered based on argFileExt\n",
    "def fnGetFullFilenames(argParentPath, argFileExt, argDebug = False):\n",
    "    lstFullFilenames = []\n",
    "    \n",
    "    # Loop recursively using argParentPath as the starting point\n",
    "    for strRoot, lstDirs, lstFilenames in os.walk(argParentPath):\n",
    "        if (argDebug):\n",
    "            print('strRoot = {}'.format(strRoot))\n",
    "            print('lstDirs = {}'.format(lstDirs))\n",
    "            print('lstFilenames = {}'.format(lstFilenames))\n",
    "            print()\n",
    "            \n",
    "        if (len(lstFilenames) > 0):\n",
    "            lstFilenames.sort()\n",
    "            \n",
    "            # Loop through the list of filenames and reconstruct\n",
    "            # with their full paths\n",
    "            for strFilename in lstFilenames:\n",
    "                # Process only files with extension argFileExt\n",
    "                if strFilename.endswith(argFileExt):\n",
    "                    # Append full path to each filename\n",
    "                    strFullFilename = os.path.join(strRoot, strFilename)\n",
    "                    lstFullFilenames.append(strFullFilename)\n",
    "                    if (argDebug): print('  {}'.format(strFullFilename))\n",
    "                    \n",
    "    lstFullFilenames.sort()\n",
    "    \n",
    "    return lstFullFilenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d20c05-0fa1-4e6a-b29c-f1863c2c91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "\n",
    "from integrated_cell import model_utils, utils\n",
    "from integrated_cell.metrics.embeddings_reference import get_latent_embeddings\n",
    "from integrated_cell.models.bvae import kl_divergence\n",
    "\n",
    "\n",
    "def dim_klds(mus, sigmas):\n",
    "    kl_dims = list()\n",
    "    for mu, sigma in zip(mus, sigmas):\n",
    "        _, kl_dim, _ = kl_divergence(mu.unsqueeze(0), sigma.unsqueeze(0))\n",
    "        \n",
    "        kl_dims.append(kl_dim)\n",
    "    \n",
    "    return np.vstack(np.vstack(kl_dims))    \n",
    "    \n",
    "    \n",
    "def get_embeddings_for_model(suffix, model_dir, parent_dir, save_path, use_current_results, mode = \"validate\"):\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        if use_current_results:\n",
    "            return None\n",
    "            \n",
    "        networks, dp, args = utils.load_network_from_dir(model_dir, parent_dir, suffix = suffix)\n",
    "\n",
    "        recon_loss = utils.load_losses(args)['crit_recon']\n",
    "\n",
    "        enc = networks['enc']\n",
    "        dec = networks['dec']\n",
    "\n",
    "        enc.train(False)\n",
    "        dec.train(False)\n",
    "\n",
    "        embeddings = get_latent_embeddings(enc, dec, dp, modes=[mode], recon_loss = recon_loss, batch_size = 32)\n",
    "\n",
    "        torch.save(embeddings, save_path)\n",
    "    else:\n",
    "        embeddings = torch.load(save_path)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def embeddings2elbo(embeddings, alpha=0.5, mode = \"validate\"):\n",
    "\n",
    "    recon_per_point = torch.mean(embeddings[mode]['ref']['recon'], 1)\n",
    "    kld_per_point =  embeddings[mode]['ref']['kld']\n",
    "    \n",
    "    elbo_per_point = -2*((1-alpha)*recon_per_point + alpha*kld_per_point)\n",
    "    \n",
    "    return elbo_per_point, recon_per_point, kld_per_point\n",
    "\n",
    "\n",
    "def get_embeddings_for_dir(model_dir, parent_dir, use_current_results=False, suffixes = None, mode = 'validate'):\n",
    "    model_paths = np.array(natsorted(glob.glob('{}/ref_model/enc_*'.format(model_dir))))\n",
    "    \n",
    "    inds = np.linspace(0, len(model_paths)-1).astype('int')\n",
    "    \n",
    "    model_paths = model_paths[inds]\n",
    "    \n",
    "    if suffixes is None:\n",
    "        suffixes = [model_path.split('/enc')[1].split('.pth')[0] for model_path in model_paths]\n",
    "    \n",
    "    results_dir = '{}/results'.format(model_dir)\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    \n",
    "    embeddings_list = list()\n",
    "    \n",
    "    logger_file = '{0}/ref_model/logger_tmp.pkl'.format(model_dir)\n",
    "    \n",
    "    if not os.path.exists(logger_file):\n",
    "        return\n",
    "    \n",
    "    with open( logger_file, \"rb\" ) as fp:\n",
    "        logger = pickle.load(fp)\n",
    "\n",
    "    args_file = \"{}/args.json\".format(model_dir)\n",
    "    with open(args_file, \"r\") as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    model_summaries = list()\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        \n",
    "        model_summary_path = \"{}/ref_model/embeddings_{}{}_summary.pth\".format(model_dir, mode, suffix)\n",
    "        \n",
    "#         if os.path.exists(model_summary_path):\n",
    "#             with open(model_summary_path, \"rb\") as f:\n",
    "#                 model_summary = pickle.load(f)\n",
    "#         else:\n",
    "        embeddings_path = \"{}/ref_model/embeddings_{}{}.pth\".format(model_dir, mode, suffix)\n",
    "    \n",
    "        embeddings = get_embeddings_for_model(suffix, model_dir, parent_dir, embeddings_path, use_current_results, mode = mode)\n",
    "\n",
    "        if embeddings is None: continue\n",
    "\n",
    "        opt = json.load(open( '{0}/args.json'.format(model_dir), \"rb\" ))\n",
    "\n",
    "        iteration = int(suffix[1:])-1\n",
    "        iteration_index = np.where(np.array(logger.log['iter']) == iteration)[0]\n",
    "\n",
    "        if len(iteration_index) == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        embeddings['beta'] = opt['kwargs_model']['alpha']\n",
    "        embeddings['elbo'], embeddings['recon'], embeddings['kld'] = embeddings2elbo(embeddings, embeddings['beta'], mode = mode)\n",
    "\n",
    "        klds_per_dim = dim_klds(embeddings[mode]['ref']['mu'], embeddings[mode]['ref']['sigma'])\n",
    "\n",
    "        model_summary = {\"iteration\": iteration,\n",
    "                \"epoch\": np.array(logger.log['epoch'])[iteration_index],\n",
    "                \"elbo\": np.mean(embeddings['elbo'].numpy()),\n",
    "                \"recons\": np.mean(embeddings['recon'].numpy()),\n",
    "                \"klds\": np.mean(embeddings['kld'].numpy()),\n",
    "                \"klds_per_dim\": np.mean(klds_per_dim, 0),\n",
    "                \"model_dir\": model_dir,\n",
    "                \"label\": model_dir.split('/')[-2],\n",
    "                \"suffix\": suffix,\n",
    "                \"args\": args}\n",
    "\n",
    "        with open(model_summary_path, \"wb\") as f:\n",
    "            pickle.dump(model_summary, f)\n",
    "\n",
    "        model_summaries.append(model_summary)\n",
    "            \n",
    "    return model_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f43b22-19ed-4ce0-bb20-447e12970eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "import importlib as imp\n",
    "import integrated_cell.utils.features\n",
    "imp.reload(integrated_cell.utils)\n",
    "imp.reload(integrated_cell.utils.features)\n",
    "\n",
    "from integrated_cell.utils.features import im2feats\n",
    "from aicsfeature.extractor.common import get_shape_features, get_intensity_features, get_skeleton_features\n",
    "\n",
    "from scipy import ndimage\n",
    "from skimage import data as skdata\n",
    "#from skimage.filters import threshold_otsu, threshold_local, try_all_threshold\n",
    "from skimage import filters, morphology, measure, color\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reshape_subplots(fig, ax):\n",
    "    num_subplots = len(ax.flatten())\n",
    "    \n",
    "    gs = matplotlib.gridspec.GridSpec(1, num_subplots)\n",
    "\n",
    "    for index in range(num_subplots):\n",
    "        ax[index].set_position(gs[index].get_position(fig))\n",
    "        \n",
    "    return\n",
    "\n",
    "def process_binmask(binmask):\n",
    "    # Generate label for unconnected objects in the binary mask\n",
    "    labels = measure.label(binmask)\n",
    "    assert(labels.max() != 0)  # There should be at least 1 CC\n",
    "    \n",
    "    # Use regionprops to extract the properties of each object\n",
    "    region_props = measure.regionprops(labels)\n",
    "    \n",
    "    # Sort the areas of all objecs and get the label of the\n",
    "    # largest one\n",
    "    areas = [(obj_prop['label'], obj_prop['area']) for obj_prop in region_props]\n",
    "    sorted_areas = sorted(areas, key=lambda x:x[1], reverse=True)\n",
    "    max_label = sorted_areas[0][0]  # Get the label of the largest object\n",
    "    \n",
    "    # Mask out all objects except for the largest one\n",
    "    single_mask = np.where(labels == max_label, 1, 0)\n",
    "    \n",
    "    # Fill in any holes in the largest object\n",
    "    filled_mask = ndimage.binary_fill_holes(single_mask)\n",
    "    \n",
    "    return filled_mask    \n",
    "    \n",
    "def get_binmask(im, method='local_mean', local_block_size=199, debug=False):\n",
    "    # methods = {'gt_zero', otsu', 'local_gaussian', 'local_mean', 'local_median', 'li', 'mean', 'all'}\n",
    "    \n",
    "    if (method == 'gt_zero' or method == 'all'):\n",
    "        # Estimate the optimal threshold\n",
    "        flt_threshold_gt_zero = 0\n",
    "        \n",
    "        # Threshold the image to get the binary mask\n",
    "        im_binmask_gt_zero = im > flt_threshold_gt_zero\n",
    "        \n",
    "        # Process the binary mask (e.g. keep the largest object, fill holes)\n",
    "        im_binmask_gt_zero = process_binmask(im_binmask_gt_zero)\n",
    "        im_binmask = im_binmask_gt_zero\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'otsu' or method == 'all'):\n",
    "        # Estimate the optimal threshold\n",
    "        flt_threshold_otsu = filters.threshold_otsu(im)\n",
    "        \n",
    "        # Threshold the image to get the binary mask\n",
    "        im_binmask_otsu = im > flt_threshold_otsu\n",
    "        \n",
    "        # Process the binary mask (e.g. keep the largest object, fill holes)\n",
    "        im_binmask_otsu = process_binmask(im_binmask_otsu)\n",
    "        im_binmask = im_binmask_otsu\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'local_gaussian' or method == 'all'):\n",
    "        flt_threshold_local_gaussian = filters.threshold_local(im, local_block_size, 'gaussian')\n",
    "        im_binmask_local_gaussian = im > flt_threshold_local_gaussian\n",
    "        im_binmask_local_gaussian = process_binmask(im_binmask_local_gaussian)\n",
    "        im_binmask = im_binmask_local_gaussian\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'local_mean' or method == 'all'):\n",
    "        flt_threshold_local_mean = filters.threshold_local(im, local_block_size, 'mean')\n",
    "        im_binmask_local_mean = im > flt_threshold_local_mean\n",
    "        im_binmask_local_mean = process_binmask(im_binmask_local_mean)\n",
    "        im_binmask = im_binmask_local_mean\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'local_median' or method == 'all'):\n",
    "        flt_threshold_local_median = filters.threshold_local(im, local_block_size, 'median')\n",
    "        im_binmask_local_median = im > flt_threshold_local_median\n",
    "        im_binmask_local_median = process_binmask(im_binmask_local_median)\n",
    "        im_binmask = im_binmask_local_median\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'li' or method == 'all'):\n",
    "        flt_threshold_li = filters.threshold_li(im)\n",
    "        im_binmask_li = im > flt_threshold_li\n",
    "        im_binmask_li = process_binmask(im_binmask_li)\n",
    "        im_binmask = im_binmask_li\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'mean' or method == 'all'):\n",
    "        flt_threshold_mean = filters.threshold_mean(im)\n",
    "        im_binmask_mean = im > flt_threshold_mean\n",
    "        im_binmask_mean = process_binmask(im_binmask_mean)\n",
    "        im_binmask = im_binmask_mean\n",
    "        if (debug): print(f'segmentation method = {method}')\n",
    "        \n",
    "    if (method == 'all'):\n",
    "        return im_binmask_gt_zero, im_binmask_otsu, im_binmask_local_gaussian, im_binmask_local_mean, im_binmask_local_median, im_binmask_li, im_binmask_mean\n",
    "    else:\n",
    "        return im_binmask\n",
    "\n",
    "def save_feats(im, save_path, seg_method='local_mean', mask_intensity_features=True, save_imgs=False, figsize_hist=(30, 4), figsize_cells=(30, 4), dpi=100, debug=False):\n",
    "    \n",
    "    assert im.shape[0] == 1 \n",
    "    \n",
    "    im_tmp = im[0].cpu().numpy()\n",
    "    if debug:\n",
    "        print(f'im_tmp.shape = {im_tmp.shape}')\n",
    "        #imshow2ch(im_tmp)\n",
    "\n",
    "    im_tmp = np.expand_dims(im_tmp, 3)\n",
    "    if debug: print(f'im_tmp.shape = {im_tmp.shape}')\n",
    "\n",
    "    im_struct = np.copy(im_tmp)\n",
    "    if debug: print(f'im_struct.shape = {im_struct.shape}')\n",
    "    \n",
    "    for i in range(im_struct.shape[0]):\n",
    "        if debug: print(f'np.max(im_struct[{i}]) = {np.max(im_struct[i])}')\n",
    "        im_struct[i] = (im_struct[i] / np.max(im_struct[i]))*255\n",
    "        \n",
    "    im_struct = im_struct.astype('uint8')\n",
    "    if debug: print(f'im_struct.shape = {im_struct.shape}')\n",
    "    \n",
    "    #if debug:\n",
    "    #    imshow2ch(im_struct[:, :, :, 0]>0)\n",
    "    #    imshow2ch(im_struct[:, :, :, 0])\n",
    "        \n",
    "    feats = {}\n",
    "    \n",
    "    if (seg_method == 'all'):\n",
    "        cell_binmask_gt_zero, cell_binmask_otsu, cell_binmask_local_gaussian, cell_binmask_local_mean, cell_binmask_local_median, cell_binmask_li, cell_binmask_mean = get_binmask(im_struct[0, :, :, 0], method=seg_method, debug=debug)\n",
    "        nuc_binmask_gt_zero, nuc_binmask_otsu, nuc_binmask_local_gaussian, nuc_binmask_local_mean, nuc_binmask_local_median, nuc_binmask_li, nuc_binmask_mean = get_binmask(im_struct[1, :, :, 0], method=seg_method, debug=debug)\n",
    "        \n",
    "        # Use local_mean method for feature extraction\n",
    "        cell_binmask = cell_binmask_local_mean\n",
    "        nuc_binmask = nuc_binmask_local_mean\n",
    "        \n",
    "    else:\n",
    "        cell_binmask = get_binmask(im_struct[0, :, :, 0], method=seg_method, debug=debug)\n",
    "        nuc_binmask = get_binmask(im_struct[1, :, :, 0], method=seg_method, debug=debug)\n",
    "    \n",
    "    #feats['dna_shape'] = get_shape_features(seg=im_struct[1]>0)\n",
    "    feats['dna_shape'] = get_shape_features(seg=nuc_binmask[:, :, np.newaxis])\n",
    "    if debug: print(f\"dna_shape = {feats['dna_shape']}\")\n",
    "        \n",
    "    if (mask_intensity_features):\n",
    "        feats['dna_inten'] = get_intensity_features(img=np.where(nuc_binmask[:, :, np.newaxis] > 0, im_struct[1], 0))  # Apply binary mask to image\n",
    "    else:\n",
    "        feats['dna_inten'] = get_intensity_features(img=im_struct[1])\n",
    "    if debug: print(f\"dna_inten = {feats['dna_inten']}\")\n",
    "    \n",
    "    #try:\n",
    "    #    feats['dna_skeleton'] = get_skeleton_features(seg=im_struct[1])\n",
    "    #except:\n",
    "    #    feats['cell_skeleton'] = {}\n",
    "    #if debug: print(f\"dna_skeleton = {feats['dna_skeleton']}\")\n",
    "\n",
    "    #feats['cell_shape'] = get_shape_features(seg=im_struct[0]>0)\n",
    "    feats['cell_shape'] = get_shape_features(seg=cell_binmask[:, :, np.newaxis])\n",
    "    if debug: print(f\"cell_shape = {feats['cell_shape']}\")\n",
    "        \n",
    "    if (mask_intensity_features):\n",
    "        feats['cell_inten'] = get_intensity_features(img=np.where(cell_binmask[:, :, np.newaxis] > 0, im_struct[0], 0))  # Apply binary mask to image\n",
    "    else:\n",
    "        feats['cell_inten'] = get_intensity_features(img=im_struct[0])\n",
    "    if debug: print(f\"cell_inten = {feats['cell_inten']}\")\n",
    "    \n",
    "    #try:\n",
    "    #    feats['cell_skeleton'] = get_skeleton_features(seg=im_struct[0])\n",
    "    #except:\n",
    "    #    feats['cell_skeleton'] = {}\n",
    "    #if debug: print(f\"cell_skeleton = {feats['cell_skeleton']}\")\n",
    "#     feats = im2feats(im_struct[0], im_struct[1], im_struct, extra_features=[\"io_intensity\", \"bright_spots\", \"intensity\", \"skeleton\"])\n",
    "    \n",
    "    if (save_imgs):\n",
    "        # Show and save histograms of input images (before and after normalization)\n",
    "        objFig_hist, objAxes_hist = plt.subplots(1, 4, figsize=figsize_hist)\n",
    "\n",
    "        objAxes_hist[0].hist(im_tmp[0, :, :, 0].flatten())\n",
    "        objAxes_hist[0].set_yscale('log')\n",
    "        objAxes_hist[0].set_title('cell_img')\n",
    "\n",
    "        objAxes_hist[1].hist(im_tmp[1, :, :, 0].flatten())\n",
    "        objAxes_hist[1].set_title('nuc_img')\n",
    "        objAxes_hist[1].set_yscale('log')\n",
    "\n",
    "        objAxes_hist[2].hist(im_struct[0, :, :, 0].flatten())\n",
    "        objAxes_hist[2].set_yscale('log')\n",
    "        objAxes_hist[2].set_title('cell_img_norm')\n",
    "\n",
    "        objAxes_hist[3].hist(im_struct[1, :, :, 0].flatten())\n",
    "        objAxes_hist[3].set_title('nuc_img_norm')\n",
    "        objAxes_hist[3].set_yscale('log')\n",
    "\n",
    "        plt.savefig(save_path.replace('.pkl', '_hist.png'), bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "        \n",
    "        if (seg_method == 'all'):\n",
    "            # Show and save input images (before and after segmentation)\n",
    "            objFig, objAxes = plt.subplots(1, 16, figsize=figsize_cells)\n",
    "\n",
    "            objAxes[0].imshow(im_tmp[0, :, :, 0], cmap='gray')\n",
    "            objAxes[0].set_title('cell_img')\n",
    "\n",
    "            objAxes[1].imshow(im_tmp[1, :, :, 0], cmap='gray')\n",
    "            objAxes[1].set_title('nuc_img')\n",
    "\n",
    "            #objAxes[2].imshow(im_struct[0, :, :, 0], cmap='gray')\n",
    "            #objAxes[2].set_title('cell_img_norm')\n",
    "\n",
    "            #objAxes[3].imshow(im_struct[1, :, :, 0], cmap='gray')\n",
    "            #objAxes[3].set_title('nuc_img_norm')\n",
    "\n",
    "            #objAxes[2].imshow(im_struct[0, :, :, 0] > 0, cmap='gray')\n",
    "            objAxes[2].imshow(cell_binmask_gt_zero, cmap='gray')\n",
    "            objAxes[2].set_title('> 0')\n",
    "\n",
    "            #objAxes[3].imshow(im_struct[1, :, :, 0] > 0, cmap='gray')\n",
    "            objAxes[3].imshow(nuc_binmask_gt_zero, cmap='gray')\n",
    "            objAxes[3].set_title('> 0')\n",
    "\n",
    "            objAxes[4].imshow(cell_binmask_otsu, cmap='gray')\n",
    "            objAxes[4].set_title('Otsu')\n",
    "\n",
    "            objAxes[5].imshow(nuc_binmask_otsu, cmap='gray')\n",
    "            objAxes[5].set_title('Otsu')\n",
    "\n",
    "            objAxes[6].imshow(cell_binmask_local_gaussian, cmap='gray')  # Good\n",
    "            objAxes[6].set_title('Local (Gaussian)')\n",
    "\n",
    "            objAxes[7].imshow(nuc_binmask_local_gaussian, cmap='gray')\n",
    "            objAxes[7].set_title('Local (Gaussian)')\n",
    "\n",
    "            objAxes[8].imshow(cell_binmask_local_mean, cmap='gray')  # Good\n",
    "            objAxes[8].set_title('Local (Mean)')\n",
    "\n",
    "            objAxes[9].imshow(nuc_binmask_local_mean, cmap='gray')\n",
    "            objAxes[9].set_title('Local (Mean)')\n",
    "\n",
    "            objAxes[10].imshow(cell_binmask_local_median, cmap='gray')\n",
    "            objAxes[10].set_title('Local (Median)')\n",
    "\n",
    "            objAxes[11].imshow(nuc_binmask_local_median, cmap='gray')\n",
    "            objAxes[11].set_title('Local (Median)')\n",
    "\n",
    "            objAxes[12].imshow(cell_binmask_li, cmap='gray')\n",
    "            objAxes[12].set_title('Li')\n",
    "\n",
    "            objAxes[13].imshow(nuc_binmask_li, cmap='gray')\n",
    "            objAxes[13].set_title('Li')\n",
    "\n",
    "            objAxes[14].imshow(cell_binmask_mean, cmap='gray')\n",
    "            objAxes[14].set_title('Mean')\n",
    "\n",
    "            objAxes[15].imshow(nuc_binmask_mean, cmap='gray')\n",
    "            objAxes[15].set_title('Mean')\n",
    "            \n",
    "        else:\n",
    "            # Show and save input images (before and after segmentation)\n",
    "            objFig, objAxes = plt.subplots(1, 4, figsize=figsize_cells)\n",
    "\n",
    "            objAxes[0].imshow(im_tmp[0, :, :, 0], cmap='gray')\n",
    "            objAxes[0].set_title('cell_img')\n",
    "\n",
    "            objAxes[1].imshow(im_tmp[1, :, :, 0], cmap='gray')\n",
    "            objAxes[1].set_title('nuc_img')\n",
    "\n",
    "            #objAxes[2].imshow(im_struct[0, :, :, 0], cmap='gray')\n",
    "            #objAxes[2].set_title('cell_img_norm')\n",
    "\n",
    "            #objAxes[3].imshow(im_struct[1, :, :, 0], cmap='gray')\n",
    "            #objAxes[3].set_title('nuc_img_norm')\n",
    "\n",
    "            #objAxes[2].imshow(im_struct[0, :, :, 0] > 0, cmap='gray')\n",
    "            objAxes[2].imshow(cell_binmask, cmap='gray')\n",
    "            objAxes[2].set_title(seg_method)\n",
    "\n",
    "            #objAxes[3].imshow(im_struct[1, :, :, 0] > 0, cmap='gray')\n",
    "            objAxes[3].imshow(nuc_binmask, cmap='gray')\n",
    "            objAxes[3].set_title(seg_method)            \n",
    "\n",
    "        for objAxis in objAxes:\n",
    "            objAxis.axis('off')\n",
    "\n",
    "        plt.savefig(save_path.replace('.pkl', '_img.png'), bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\n",
    "        # NOTE: Li and mean thresholds give good results\n",
    "        #fig, ax = filters.try_all_threshold(im_struct[0, :, :, 0], figsize=(20, 4), verbose=False)\n",
    "        #reshape_subplots(fig, ax)\n",
    "        #plt.savefig(save_path.replace('.pkl', '_cellseg.png'), bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\n",
    "        #fig, ax = filters.try_all_threshold(im_struct[1, :, :, 0], figsize=(20, 4), verbose=False)\n",
    "        #reshape_subplots(fig, ax)\n",
    "        #plt.savefig(save_path.replace('.pkl', '_nucseg.png'), bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\n",
    "        if (not fnIsBatchMode()):\n",
    "            plt.show()\n",
    "\n",
    "        plt.close('all')\n",
    "    \n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(feats, f)\n",
    "    \n",
    "    return\n",
    "\n",
    "import re\n",
    "\n",
    "def load_feats(save_paths):\n",
    "    feats = list()\n",
    "    for save_path in save_paths:\n",
    "        with open(save_path, 'rb') as f:\n",
    "            feat_tmp = pickle.load(f)\n",
    "            \n",
    "        feat = {}\n",
    "        \n",
    "        # Extract the cell_idx from the filename\n",
    "        _, basename, _ = fnSplitFullFilename(save_path)\n",
    "        result = re.match('feat_(\\d+)', basename)\n",
    "        cell_idx = result.groups()[0]\n",
    "        feat['cell_idx'] = cell_idx\n",
    "        \n",
    "        for i in feat_tmp:\n",
    "            for j in feat_tmp[i]:\n",
    "                feat[\"{}_{}\".format(i,j)] = feat_tmp[i][j]            \n",
    "            \n",
    "        feats.append(feat)\n",
    "\n",
    "    feats = pd.DataFrame.from_dict(feats)\n",
    "        \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcd1ec-b810-4067-a085-7d4bb9a59002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import integrated_cell.metrics.embeddings_reference as get_embeddings_reference\n",
    "\n",
    "def fnLoadUnsortedEmbeddings(argUnsortedEmbeddingsPath = './dctUnsortedEmbeddings.pth', argEncoder = None, argDecoder = None, argDataProvider = None, argArgs = None, argReGen = False):\n",
    "    # See if the embedding file already exists. If so, load it\n",
    "    if (os.path.exists(argUnsortedEmbeddingsPath) and not(argReGen)):\n",
    "        print('Loading unsorted embeddings file from: {}'.format(argUnsortedEmbeddingsPath))\n",
    "        embeddings_ref_untreated_ref_mu = torch.load(argUnsortedEmbeddingsPath)\n",
    "        \n",
    "    # If the embedding file does not exist, generate one\n",
    "    else:\n",
    "        print('Generating unsorted embeddings file at: {}'.format(argUnsortedEmbeddingsPath))\n",
    "        \n",
    "        ref_enc = argEncoder\n",
    "        ref_dec = argDecoder\n",
    "        dp_ref  = argDataProvider\n",
    "\n",
    "        recon_loss = utils.load_losses(argArgs)['crit_recon']\n",
    "        batch_size = dp_ref.batch_size\n",
    "        \n",
    "        ### get embeddings for all cells and save to dict\n",
    "        embeddings_ref_untreated_val = get_embeddings_reference.get_latent_embeddings(\n",
    "            ref_enc,\n",
    "            ref_dec,\n",
    "            dp_ref,\n",
    "            recon_loss,\n",
    "            batch_size=batch_size,\n",
    "            modes=['validate'],\n",
    "        )\n",
    "        embeddings_ref_untreated_test = get_embeddings_reference.get_latent_embeddings(\n",
    "            ref_enc,\n",
    "            ref_dec,\n",
    "            dp_ref,\n",
    "            recon_loss,\n",
    "            batch_size=batch_size,\n",
    "            modes=['test'],\n",
    "        )\n",
    "        embeddings_ref_untreated_train = get_embeddings_reference.get_latent_embeddings(\n",
    "            ref_enc,\n",
    "            ref_dec,\n",
    "            dp_ref,\n",
    "            recon_loss,\n",
    "            batch_size=batch_size,\n",
    "            modes=['train'],\n",
    "        )\n",
    "        embeddings_ref_untreated_ref_mu = {\n",
    "            \"train\": embeddings_ref_untreated_train[\"train\"][\"ref\"][\"mu\"],\n",
    "            \"test\": embeddings_ref_untreated_test[\"test\"][\"ref\"][\"mu\"],\n",
    "            \"validate\": embeddings_ref_untreated_val[\"validate\"][\"ref\"][\"mu\"],\n",
    "        }\n",
    "\n",
    "        torch.save(embeddings_ref_untreated_ref_mu, argUnsortedEmbeddingsPath)\n",
    "        \n",
    "    return embeddings_ref_untreated_ref_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145e46e-a998-465b-80e3-3d06572b854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def imshow2ch(im):\n",
    "    objFig, (objAx1, objAx2) = plt.subplots(1, 2, figsize=(8, 5))\n",
    "    objAx1.imshow(im[0, :, :], cmap='gray')\n",
    "    objAx2.imshow(im[1, :, :], cmap='gray')\n",
    "    \n",
    "def fnGenUnsortedEmbeddingsPath(argEmbeddingsParentPath, argModelDir, argRefSuffix, argNumPathLevels=2):\n",
    "    strModelName = '_'.join(re.sub('\\/\\/+', '/', argModelDir.strip('/').replace(':', '-')).split('/')[-argNumPathLevels:])  # Replace multiple slashes with single slash and split the path\n",
    "    return argEmbeddingsParentPath + 'dctUnsortedEmbeddings_2DModel_' + strModelName + argRefSuffix + '.pth'\n",
    "    \n",
    "def fnGenRandomZ(argEmbeddingsParentPath, argModelDir, argRefSuffix, argBatchSize=1, argMode='train'):\n",
    "    strEmbeddingsFullFilename = fnGenUnsortedEmbeddingsPath(argEmbeddingsParentPath, argModelDir, argRefSuffix)\n",
    "    dctUnsortedEmbeddings = fnLoadUnsortedEmbeddings(strEmbeddingsFullFilename)\n",
    "    embeddings_unfiltered = dctUnsortedEmbeddings[argMode]\n",
    "    print(f'embeddings_unfiltered.shape = {embeddings_unfiltered.shape}')\n",
    "    \n",
    "    arrEmbeddingsMean = torch.mean(embeddings_unfiltered, dim=0, keepdim=True)\n",
    "    arrEmbeddingsStdev = torch.std(embeddings_unfiltered, dim=0, keepdim=True)\n",
    "    #print(f'{arrEmbeddingsMean.shape}, {arrEmbeddingsStdev.shape}')\n",
    "    \n",
    "    arrRandomZ = torch.normal(mean=arrEmbeddingsMean.repeat(argBatchSize, 1), std=arrEmbeddingsStdev.repeat(argBatchSize, 1))\n",
    "    \n",
    "    return arrRandomZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0546c49-a7d8-459d-b65f-6f32b7ab9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_img_type = {\n",
    "    'hist': '_hist.png', \n",
    "    'cell': '_img.png', \n",
    "}\n",
    "\n",
    "def gen_feature_path(feats_parent_dir, intensity_norm, beta, sampling):\n",
    "    fn_norm = 'norm_' + str(intensity_norm)\n",
    "    \n",
    "    # If beta is 'real', there is no sampling\n",
    "    if (beta == 'real'):\n",
    "        fn_jobname = 'feats_test'\n",
    "        fn_sampling = ''\n",
    "    else:\n",
    "        df_master = pd.read_csv('/allen/aics/modeling/caleb/data/df_master.csv')\n",
    "        model_dir = df_master.query('beta == @beta & intensity_norm == @intensity_norm')['model_dir'].to_numpy()[0]\n",
    "        \n",
    "        fn_jobname = os.path.basename(os.path.normpath(model_dir))\n",
    "        fn_sampling = sampling\n",
    "        \n",
    "    return os.path.join(feats_parent_dir, fn_norm, fn_jobname, fn_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d821db-538e-4be4-941d-34c7b71465dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(in_dict, dict_out=None, parent_key=None, separator=\"_\"):\n",
    "    if dict_out is None:\n",
    "        dict_out = {}\n",
    "\n",
    "    for k, v in in_dict.items():\n",
    "        k = str(k)\n",
    "        #print(f'{k}')\n",
    "        k = f\"{parent_key}{separator}{k}\" if parent_key else k\n",
    "        #print(f'  {k}')\n",
    "        if isinstance(v, dict):\n",
    "            #print(f'  Recursing...')\n",
    "            flatten_dict(in_dict=v, dict_out=dict_out, parent_key=k)\n",
    "            continue\n",
    "\n",
    "        #print(f'  dct = {k}')\n",
    "        dict_out[k] = v\n",
    "\n",
    "    return dict_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
