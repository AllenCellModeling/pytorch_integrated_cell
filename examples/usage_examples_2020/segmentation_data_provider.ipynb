{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "\n",
    "from integrated_cell import model_utils, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_ids = [2]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(ID) for ID in gpu_ids])\n",
    "if len(gpu_ids) == 1:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "    \n",
    "image_parent = '/allen/aics/modeling/gregj/results/ipp/scp_19_04_10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "df = pd.read_csv(Path(image_parent) / \"data_jobs_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(df.sample().save_reg_path.item()).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aicsimageio import AICSImage\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = AICSImage(df.sample().save_reg_path.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.get_channel_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 64, 168, 104)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = img.get_image_data(\"CZYX\", S=0, T=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "foo = img.data[0,0,c,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGgAAACoCAAAAAApn/TJAAAE2UlEQVR4nO2aXWxURRiGX1qxrWWxVtpqNYItIq0/aBEqCdYYpWopEqAxShNuFMGY1GhNavwJRuEGRTAGJRBDUIiKf20TuOBCjeFCESyVEloMxaIgtqW1pT/WYl8vut3t7pnZnW/29Mp5bnb37Dnf831z5syZM7uAw+FwOBwOh8Ph+L9xL4D0idcsOcLmisYDOyqaqhbOnDhNwackyRGSZFf71CsmRpO8gZE01CH7vaULAgCAGx72TVRMD7t6eamrfxaAl4YHcn3ylHzt8Yy2IT/G0sApcrsPkmtmXLvdW0+I79lKkiUJOVanPb/4hb7mgRieMb5JxLOV52kiIUk+Ye8JdmgdI5Eff0sJWHrWG5QxXtayF7cHbK6uVw3bLCyr5dH9NiU1m7dbmJUWoscMS4rg7KEnf9r6rEy09t94UbV1rZgP5BuL3rEoaEzdENjEylRMNdBM/srSMfrmItne+Xr3GgNTTa9FRdHyP0zuk1PqpLUoeNMbNyl6Q986pZ+eLZP0ybLXQIQLJ1VxFWG97tDOGSaizFmMmW6EWyMrSs9W7R3Fivx7ysd/pok1ki2ZCw5+0Mk5n8Xbcea6xPseTw+/bJDTL8Koij64c7pJ8SvjhI3VuUk2Llp9v4kGwH5hSZHs8cTz9rpRsrIME1Kzw1jU8WdCohrzXe9KpOUal3niXaYTpXZlWtUCAPvKvdt0TYfp9h6c0kZVoBjER6JeNfywUJTVndYnqFbkATZZmypkormDihjKVove2CTRJJtMWXW8KBCVqpOO0w+CDEme0L5NoKK3Usw9cwyS1+1xWlAPsDdeOA2Xeo4tF4kKZfFDDC4SaQDstPJ0Xif15FhWVC0VPaS6Xg3om6aOpx1nW1OlqY2y+YLwgNzj0v5GkmzRxdNWlF5oNF2N5nP5IfVWp0g7oGoryl0izw3Ar7rRRys6tzleTOUEP2/IMKMQGUOkePwhm2ZLRcUdFmfob/2dSDvdyrlan4TuQeaf+T9rj9Gdo6q6GJ1b99WZAf0xOtFNAADGeH5UkNMj2bv0ubkoO2Rxgkg+IvDMIHnQTsP+KyUVvW1pIX9cLPFgaju9sx6T60nf4wBVZ+j9aFz/jX6NxW3CGWqKzZVKkudvkVVUFVoFi921Pd/miETLGjdePvZe316EYuFkOCNmYhEk77ZsNpJUPOZpK0oKT5rNh4SxPZ8WiKYcDfVR8/t4cM9zXxofAQDl1i33vsgDbGmyNd0tE822LuldYUnrLW7gJLuF6+u4Xu4YIUn1UmwYz8jw+xvCzEa73cV94sOSWi0aji0FYhFKKmsHumXtxhNyDQAgb7m0okfjhVRPt1pby4pvfvAq5XfKuVZXm6iQSNI+NC/obF4CImTfarpWfMTyqS1EpaGoPi1uKO2UGACwp/+BZ0wSOjNolngMpv1lVFL8ATXeomTnazG/HrvpFRllHZvDx0mSDYf19QyXZfggyi8g2VeNeT36X9BPHvBBBGzgthwA6VVR4cffTVb5IUqeF3xT+ImmohFu9EMU5sbH23hsd1vYcKKLJPndff56AKyqwbgliI7SO6qb12x7yndNkKKgpzcLwGSrFRZDgs9Q9X79jUZLoJ3NX3THvQf5wNpXMKnF6Dc8H5igv1c5HA6Hw+FwOBwOh8PhcDgcpvwHdnrfGbTDPv0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=104x168 at 0x7FB02D75DC50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(foo.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from integrated_cell.data_providers.DataProvider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading csv manifest\n"
     ]
    }
   ],
   "source": [
    "dp = DataProvider(\n",
    "    \"/raid/shared/ipp/scp_19_04_10/\",\n",
    "    16,\n",
    "    csv_name=\"controls/data_plus_controls.csv\",\n",
    "    check_files=False,\n",
    "    channelInds=[3,1,4],\n",
    "    normalize_intensity=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = dp.get_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 104, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGgAAACoCAAAAAApn/TJAAAFb0lEQVR4nO2aTWxVVRDH5xZBtLU8glBSjUq1MS1tYhQKMRg1NkXTUE3QsKFgxERjlCYGY6LBhQs3RutSNGp0gTGilURIsJoQEeoGaOqiSkQbsLYNKUJ7H/VpfeOiX/e9OzNn5rz7XJj7X7TvnXvP/N7MmfNx7zkAqVKlSpXq/6zWO0uqfpX2xo0PNQxUVVZ9/HlJOIVeRUREPPO4Z/0K7Y3HjwEAQH1j5097PVk6NR6Ycen3kxh2lxO0ExeU+9BeXx26MPJ5SfXSajtKp9XdEY8wj0dvvqU8oE1YrD6bAW3owljJipYNNpRKlV0xjxBHtnQkzVnSSXAQ8ULSoGqag7knn0sWVMGAEPEVdQdRKcuTuq9JEjTGg/D95clxWkZi5vMLHz+7ISlOw1kBg4i99QmB+oTAISJi3x3JgHpdIPzhHpcNTXLW0vM9Rj43JdFMmYNOhxB/fbR00L68m4M4uku24g7d1RgUholWzRt7SgS99BQABLHiGHrZ9SWCsnRxHN311qKSQNPCtQK3lj77wbVOa7x2ajJhTgdXsHacHsXncEEdB2603B5Vq8UhRDx5ux9nwxUjCE8zlhyhy5lntSo/EJPbgjIbzVUAYJU0szKapE3F+11U1ZftPw7pIMmhM+X2rIJKD1C/ezCNi04HGbRajmxioC8zNsSM/x6hu2mxxvCCZvz38MjVi2KBRT/Qm40OEEFG9AhdnfSgyuRj4OOR1IuQy8fg7EVPEP3T5zjxq28fMoOyBSYLhQjMVWb4VoWOcIrvyXbQ1u2SVZZ0awNZLLxGq6mLfGEbP6ZHwk6qWPCoMOnInCAL6djxHrXsjn6jHSJLrf1o+V3sJVnWkcFn0gMAu0f2dYknyNsja+h+2+EJoj0SukcVs3ByalGeKJT60QOeINIlscNPcutbWbUjRGFJUzkj8ueJoM3nhYv8kq/d/haq3770RkSkXqP4LonFJSzVk2QQ30hiEj24xgryHB267k0SJAWPyDvf0M0Hj+Tdt84ICiOWCIuIZGMhbH04VihvvYWwYImwyGQEuVpVh45yiK9oBUWSQe8QINWTZNAnz4uXGXmE7tIvPiCwg7wXDtbQwbF2P5DZo9y5/wjkOdrl9purrPSZj6b3efw4H9Kn0ltcVtNmzlf0EtKlS5w97gX/9+QbXPd5hvA6KmEwYEagoYuPjZscmVfwoyFqk8NrGTNuj1AaHYqeOPObT3B3KvaPxv+iGQCxAbyd5ah0SBm3J6QdA82OWEjPcVhU+sL+KyWCsuS2DgYQREmvvfOnZERzXIcc74L5P7O6PCEaUYZOsSfmeMTRgC78QS2rsOCfE6RSlzPhjrTd7b2hE9G2YQfn+PoEKAAA22RO//0JcaBd5JxJ7ujJ+kGBc36724Ba6wTQgMqC8nwdv0aZeE96ojZr1QnOn7EkMQCQ4UDa9Zj2VelielICgArdHpP2VM/fOe6KcuhRHx/q4UhJjHEFGipundnl1m266voDUbFWn2nenilddfU5VepRaXA0G+4dVltQ6pt4arcZqus9KgodAgSWPPBuoyAIoKnGQNJqLXXc6Rl9fbVHu1qLSxCguY66lZS6jYqTDoMA4OllPT9zJyV89SLxPJRHxCObdPXVHoXE+BsAQNvQd6r66jZiHl4w8UGVmXcCLcgQOlJjh08lDCJCNzWYDU+/rKyvBVU2FRX8kw1PbVFWtqg+mtbn1qxM9BRsVLURzji9pStLG7qFXJjqmBgsI6h59j/uCL/2wOg1d1hst34C89PhHCLi63vKlgTz+ihEfDdTdgwADCA2u+9iZZnKv5Af8JNSL8bmWIssHvm9WJyVPlu/zY6WAkqVKlWqVKlSpUpVVv0LjqkCphdEEiIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=104x168 at 0x7FB026249690>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=8\n",
    "c=0\n",
    "img_np = a[n].numpy()\n",
    "print(img_np[c].shape)\n",
    "im_arr = np.max(img_np[c], axis=2)\n",
    "im_arr = (255*im_arr/im_arr.max()).astype(np.uint8)\n",
    "Image.fromarray(im_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_np[2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tifffile\n",
    "from PIL import Image\n",
    "from integrated_cell.utils.utils import str2rand\n",
    "\n",
    "from integrated_cell.data_providers.DataProviderABC import DataProviderABC\n",
    "\n",
    "\n",
    "class SegmentationDataProviderRef(DataProviderABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_parent=\"/allen/aics/modeling/gregj/results/ipp/scp_19_04_10/\",\n",
    "        csv_name=\"data_jobs_out.csv\",\n",
    "        image_col = \"save_reg_path\",\n",
    "        channelInds=[0,1],\n",
    "        target_col=\"StructureId/Name\",\n",
    "        batch_size=16,\n",
    "        n_dat=-1,\n",
    "        hold_out=0.1,\n",
    "        verbose=True,\n",
    "        check_files=True,\n",
    "        split_seed=1,\n",
    "        return2D=False,\n",
    "        slab_width=None,\n",
    "        rescale_to=None,\n",
    "        crop_to=None,\n",
    "        make_controls=False,\n",
    "        normalize_intensity=True,\n",
    "    ):\n",
    "\n",
    "        self.hold_out = hold_out\n",
    "        self.verbose = verbose\n",
    "        self.channelInds = channelInds\n",
    "        self.check_files = check_files\n",
    "        self.split_seed = split_seed\n",
    "        self.crop_to = crop_to\n",
    "        self.rescale_to = rescale_to\n",
    "        self.image_parent = image_parent\n",
    "        self.image_col = image_col\n",
    "        self.csv_name = csv_name\n",
    "        self.normalize_intensity = normalize_intensity\n",
    "        self.return2D = return2D\n",
    "        self.slab_width = slab_width\n",
    "        self.batch_size = batch_size\n",
    "        self.target_col = target_col\n",
    "\n",
    "        # make a dataframe out of the csv log file\n",
    "        csv_path = os.path.join(self.image_parent, self.csv_name)\n",
    "        if self.verbose:\n",
    "            print(\"reading csv manifest\")\n",
    "        csv_df = pd.read_csv(csv_path)\n",
    "        self.csv_data = csv_df\n",
    "        \n",
    "        # rewire paths to images to be relative to image_parent\n",
    "        df[self.image_col] = df[self.image_col].apply(\n",
    "            lambda p: Path(image_parent) / Path(p).relative_to(Path(p).parent.parent)\n",
    "        )\n",
    "\n",
    "        # check which rows in csv are valid, based on all the channels i want being present\n",
    "        if self.check_files:\n",
    "            if self.verbose:\n",
    "                print(\"Checking the existence of files\")\n",
    "\n",
    "            for index, row in tqdm(csv_df.iterrows(), total=len(csv_df)):\n",
    "                is_good_row = True\n",
    "\n",
    "                image_path = row[self.image_col]\n",
    "\n",
    "                try:\n",
    "                    self.load_image(row)\n",
    "                except:  # noqa\n",
    "                    print(f\"Could not load from image. {image_path}\")\n",
    "                    is_good_row = False\n",
    "\n",
    "                csv_df.loc[index, \"valid_row\"] = is_good_row\n",
    "\n",
    "            # only work with valid rows\n",
    "            n_old_rows = len(csv_df)\n",
    "            csv_df = csv_df.loc[csv_df[\"valid_row\"] == True]  # noqa\n",
    "            csv_df = csv_df.drop(\"valid_row\", 1)\n",
    "            n_new_rows = len(csv_df)\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"{n_new_rows}/{n_old_rows} samples have all files present\"\n",
    "                )\n",
    "\n",
    "        # Psuedorandomly deterministically convert the cellID\n",
    "        # to a number for train/validation/test splits\n",
    "        rand_split = str2rand(csv_df[\"CellId\"], self.split_seed)\n",
    "        rand_dna_memb = str2rand(csv_df[\"CellId\"], self.split_seed + 1)\n",
    "\n",
    "        # log rand splits to df\n",
    "        csv_df[\"rand_split\"] = rand_split\n",
    "        csv_df[\"rand_dna_memb\"] = rand_dna_memb\n",
    "\n",
    "        # reset df index\n",
    "        csv_df = csv_df.reset_index()\n",
    "        \n",
    "        # get label data\n",
    "        image_classes = list(csv_df[self.target_col])\n",
    "        self.csv_data = csv_df\n",
    "        self.image_classes = image_classes\n",
    "\n",
    "        nimgs = len(csv_df)\n",
    "\n",
    "        [label_names, labels] = np.unique(image_classes, return_inverse=True)\n",
    "        self.label_names = label_names\n",
    "\n",
    "        onehot = np.zeros((nimgs, np.max(labels) + 1))\n",
    "        onehot[np.arange(nimgs), labels] = 1\n",
    "\n",
    "        self.labels = labels\n",
    "        self.labels_onehot = onehot\n",
    "\n",
    "        # save data dict -- which indices in df are in which split\n",
    "        self.data = {\n",
    "            \"test\": {\n",
    "                \"inds\": np.where(csv_df[\"rand_split\"] <= self.hold_out)[0]\n",
    "            },\n",
    "            \"validate\": {\n",
    "                \"inds\": np.where(\n",
    "                    (csv_df[\"rand_split\"] > self.hold_out)\n",
    "                    & (csv_df[\"rand_split\"] <= self.hold_out * 2)\n",
    "                )[0]\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"inds\": np.where(csv_df[\"rand_split\"] > self.hold_out * 2)[0]\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # save out image size for later\n",
    "        self.imsize = self.load_image(csv_df.iloc[0]).shape\n",
    "\n",
    "        # initialze dict of embedding tensors\n",
    "        self.embeddings = {\n",
    "            \"train\": torch.zeros([len(self.data[\"train\"][\"inds\"]), 0]),\n",
    "            \"test\": torch.zeros([len(self.data[\"test\"][\"inds\"]), 0]),\n",
    "            \"validate\": torch.zeros(\n",
    "                [len(self.data[\"validate\"][\"inds\"]), 0]\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # i don't get this n_dat stuff\n",
    "        if n_dat == -1:\n",
    "            self.n_dat = {\"train\": len(self.data[\"train\"][\"inds\"])}\n",
    "        else:\n",
    "            self.n_dat = {\"train\": n_dat}\n",
    "        self.n_dat[\"validate\"] = len(self.data[\"validate\"][\"inds\"])\n",
    "        self.n_dat[\"test\"] = len(self.data[\"test\"][\"inds\"])\n",
    "\n",
    "        \n",
    "    def load_image(self, row):\n",
    "        aicsimg = AICSImage(row[self.image_col])\n",
    "        im_tmp = aicsimg.get_image_data(\"CZYX\", S=0, T=0)\n",
    "        im = im_tmp[self.channelInds]\n",
    "        if self.normalize_intensity:\n",
    "            im = im.astype(np.float32)\n",
    "            for i, ch in enumerate(im):\n",
    "                im[i] = ch / np.max(ch)\n",
    "        \n",
    "        if self.return2D:\n",
    "            _,z,_,_ = im.shape\n",
    "            slab_width = 8 if self.slab_width is None else self.slab_width\n",
    "            im = im[:,(z-slab_width)//2:(z+slab_width)//2,...].max(axis=1)\n",
    "            \n",
    "        return im\n",
    "\n",
    "    \n",
    "    def set_n_dat(self, n_dat, train_or_test=\"train\"):\n",
    "        if n_dat == -1:\n",
    "            self.n_dat[train_or_test] = len(self.data[train_or_test][\"inds\"])\n",
    "        else:\n",
    "            self.n_dat[train_or_test] = n_dat\n",
    "\n",
    "            \n",
    "    def get_n_dat(self, train_or_test=\"train\", override=False):\n",
    "\n",
    "        if override:\n",
    "            n_dat = len(self.data[train_or_test][\"inds\"])\n",
    "        else:\n",
    "            n_dat = self.n_dat[train_or_test]\n",
    "\n",
    "        return n_dat\n",
    "\n",
    "    \n",
    "    def __len__(self, train_or_test=\"train\"):\n",
    "        return self.get_n_dat(train_or_test)\n",
    "\n",
    "    \n",
    "    def get_image_paths(self, inds_tt, train_or_test):\n",
    "        inds_master = self.data[train_or_test][\"inds\"][inds_tt]\n",
    "        return list(self.csv_data[self.image_col][inds_master])\n",
    "\n",
    "    # this is wild\n",
    "    def get_images(self, inds_tt, train_or_test):\n",
    "        dims = list(self.imsize)\n",
    "        dims[0] = len(self.channelInds)\n",
    "        dims.insert(0, len(inds_tt))\n",
    "\n",
    "        inds_master = self.data[train_or_test][\"inds\"][inds_tt]\n",
    "\n",
    "        images = torch.zeros(tuple(dims))\n",
    "\n",
    "        for i, (rownum, row) in enumerate(self.csv_data.iloc[inds_master].iterrows()):\n",
    "            image = self.load_image(row)\n",
    "            images[i] = torch.from_numpy(image)\n",
    "\n",
    "        if self.rescale_to is not None:\n",
    "            images = torch.nn.functional.interpolate(\n",
    "                images, scale_factor=self.rescale_to\n",
    "            )\n",
    "\n",
    "        if self.crop_to is not None:\n",
    "            crop = (np.array(images.shape[2:]) - np.array(self.crop_to)) / 2\n",
    "            crop_pre = np.floor(crop).astype(int)\n",
    "            crop_post = np.ceil(crop).astype(int)\n",
    "\n",
    "            pad_pre = -crop_pre\n",
    "            pad_pre[pad_pre < 0] = 0\n",
    "\n",
    "            pad_post = -crop_post\n",
    "            pad_post[pad_post < 0] = 0\n",
    "\n",
    "            crop_pre[crop_pre < 0] = 0\n",
    "\n",
    "            crop_post[crop_post < 0] = 0\n",
    "            crop_post[crop_post == 0] = -np.array(images.shape[2:])[crop_post == 0]\n",
    "\n",
    "            if len(crop_pre) == 2:\n",
    "                images = images[\n",
    "                    :,\n",
    "                    :,\n",
    "                    crop_pre[0] : -crop_post[0],  # noqa\n",
    "                    crop_pre[1] : -crop_post[1],  # noqa\n",
    "                ]\n",
    "\n",
    "            elif len(crop_pre) == 3:\n",
    "                images = images[\n",
    "                    :,\n",
    "                    :,\n",
    "                    crop_pre[0] : -crop_post[0],  # noqa\n",
    "                    crop_pre[1] : -crop_post[1],  # noqa\n",
    "                    crop_pre[2] : -crop_post[2],  # noqa\n",
    "                ]\n",
    "\n",
    "            pad_pre = np.hstack([np.zeros(2), pad_pre])\n",
    "            pad_post = np.hstack([np.zeros(2), pad_post])\n",
    "            padding = np.vstack([pad_pre, pad_post]).transpose().astype(\"int\")\n",
    "            images = np.pad(images, padding, mode=\"constant\", constant_values=0)\n",
    "            images = torch.tensor(images)\n",
    "\n",
    "        return images\n",
    "\n",
    "    def get_classes(self, inds_tt, train_or_test, index_or_onehot=\"index\"):\n",
    "        inds_master = self.data[train_or_test][\"inds\"][inds_tt]\n",
    "\n",
    "        if index_or_onehot == \"index\":\n",
    "            labels = self.labels[inds_master]\n",
    "        else:\n",
    "            labels = np.zeros([len(inds_master), self.get_n_classes()])\n",
    "            for c, i in enumerate(inds_master):\n",
    "                labels[c, :] = self.labels_onehot[i, :]\n",
    "\n",
    "            labels = torch.from_numpy(labels).long()\n",
    "\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return labels\n",
    "\n",
    "    def get_n_classes(self):\n",
    "        return self.labels_onehot.shape[1]\n",
    "\n",
    "    def set_ref(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def get_ref(self, inds, train_or_test=\"train\"):\n",
    "        inds = torch.LongTensor(inds)\n",
    "        return self.embeddings[train_or_test][inds]\n",
    "\n",
    "    def get_n_ref(self):\n",
    "        return self.get_ref([0], \"train\").shape[1]\n",
    "\n",
    "    def get_sample(self, train_or_test=\"train\", inds=None):\n",
    "        if inds is None:\n",
    "            rand_inds = np.random.permutation(self.get_n_dat(train_or_test))\n",
    "            inds = rand_inds[0 : self.batch_size]  # noqa\n",
    "        x = self.get_images(inds, train_or_test)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading csv manifest\n"
     ]
    }
   ],
   "source": [
    "dp = SegmentationDataProviderRef(check_files=False, return2D=True, slab_width=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dp.get_sample(\"train\", inds=[0,1,2222,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  9, 12,  0])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.get_classes([0,1111,2222,6666], \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/allen/aics/modeling/gregj/results/ipp/scp_19_04_10/plate_3500001506/8383_753_reg.tiff',\n",
       " '/allen/aics/modeling/gregj/results/ipp/scp_19_04_10/plate_3500001310/6271_38369_reg.tiff']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.get_image_paths([9,999], \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 168, 104])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.get_images([33,1111, 0], \"train\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 168, 104])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.get_sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_integrated_cell]",
   "language": "python",
   "name": "conda-env-pytorch_integrated_cell-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
